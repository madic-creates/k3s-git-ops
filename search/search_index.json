{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"K3S","text":"<p>Warning</p> <p>This documentation is at a very early stage.</p> <p>This documentation provides an overview of my K3s GitOps home lab repository, explaining the high-level architecture, core principles, and system capabilities. The repository implements a fully automated Kubernetes home lab using GitOps practices with ArgoCD as the deployment orchestrator.</p> <p>For detailed information about specific subsystems, see GitOps Platform for ArgoCD configuration, Infrastructure Services for foundational components, Authentication &amp; Identity for SSO implementation, Monitoring &amp; Observability for the monitoring stack, Applications for user-facing services, and Backup &amp; Data Protection for data protection strategies.</p>"},{"location":"#gitops-principles","title":"GitOps principles","text":"<p>The cluster operates on GitOps principles with ArgoCD serving as the primary deployment controller. All infrastructure and application configurations are stored as code in the GitHub repository, with ArgoCD continuously monitoring for changes and maintaining the desired cluster state.</p>"},{"location":"#important-used-technologies","title":"Important used technologies","text":"<ul> <li> <p>K3S This is the foundation of the infrastructure. It's an easy to deploy and maintain kubernetes distribution</p> </li> <li> <p>ArgoCD is a tool to manage kubernetes clusters the GitOPs way</p> </li> <li> <p>Kustomize is used to manage the kubernetes manifests within ArgoCD</p> </li> <li> <p>Ansible prepares the machines for the k3s installation and installs k3s</p> </li> <li> <p>Vagrant manages the test environment</p> </li> </ul>"},{"location":"#features","title":"Features","text":"<p>Excerpt of features this cluster provides:</p> <ul> <li>Provision nodes, including k3s, via ansible</li> <li>GitOps based cluster management with ArgoCD</li> <li>Encrypted secrets with sops</li> <li>Every exposed service uses SSO with Authelia</li> <li>File backups from persistant volumes<ul> <li>Backup any folder to a restic supported storage backend</li> <li>Delete old backups (Daily, Weekly, Monthly, Always Keep Last)</li> <li>ntfy.sh notification on failure</li> <li>prometheus pushgateway metrics</li> </ul> </li> <li>KubeDoom: Killing whoami containers with a shotgun</li> <li>High Avaliability ControlPlane and LoadBalancer via KubeVIP</li> <li>Monitoring via kube-prometheus-stack</li> <li>Logging via loki</li> <li>Alerting via alertmanager to a selfhosted ntfy</li> <li>Storage managed via longhorn</li> <li>Vagrant based virtual test environment</li> </ul> Feature Category Components Key Capabilities High Availability KubeVIP, Traefik, Cilium HA control plane, load balancing, VIP management, Network Policies Authentication Authelia, LLDAP Single Sign-On, LDAP directory, forward authentication Storage Longhorn Distributed block storage Monitoring kube-prometheus-stack, Loki, CheckMK Metrics collection, log aggregation, alerting Backup Velero, Restic Cluster backups, application data protection Media Services Emby, NextPVR Media streaming, TV recording, transcoding Automation RenovateBot, Semaphore Dependency updates, playbook execution Security SOPS, cert-manager Secret encryption, TLS certificate management"},{"location":"#todos","title":"ToDos","text":"<ul> <li>\u2705 Backup<ul> <li>\u2705 Notification on failure</li> </ul> </li> <li>\u2705 Encryption of secrets<ul> <li>\u2705 Rework documentation</li> </ul> </li> <li> Extend Monitoring beyond kube-prometheus-stack defaults</li> <li>\u2705 Migrate renovate to github actions</li> </ul>"},{"location":"argocd/","title":"ArgoCD","text":"<p>ArgoCD serves as the central control plane for the entire cluster, implementing a pull-based deployment model where Argo CD monitors the Git repository and synchronizes the cluster state to match the declared configuration.</p>"},{"location":"argocd/#dependencies","title":"Dependencies","text":"<p>ArgoCD does not support \"real\" dependencies. Therefore, I use the ArgoCD feature sync waves. Sync waves determine the order in which ArgoCD apps are installed.</p> <p>Example: In my case argo-cd requires a Configuration Management Plugin to be installed first. Without the Configurationmanagementplugin, the argo-cd repo-server container won't start.</p> Sync Wave Range Purpose Examples 0-5 Infrastructure Foundation kubevip-ha, cert-manager 8-15 Core Services mariadb, redis, longhorn 12-17 Network &amp; DNS pihole, traefik 16-17 Monitoring monitoring, loki 20-60 Applications emby, nextpvr, semaphore 96-99 GitOps Platform argo-cd, argo-cd-apps <p>All Argo CD applications are defined in the apps/argo-cd-apps directory where the argo-cd-apps application manages all individual application definitions.</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"},{"location":"argocd/#getting-argocd-admin-password","title":"Getting ArgoCD admin password","text":"<p>This shouldn't be necessary anymore because the helm chart supports setting the password value. But I will leave it here for reference.</p> <pre><code>kubectl patch secret -n argocd argocd-secret -p '{\"stringData\": { \"admin.password\": \"'$(htpasswd -bnBC 10 \"\" ${ADMINPASSWORD} | tr -d ':\\n')'\"}}'\n</code></pre>"},{"location":"argocd/#configmanagementplugin","title":"ConfigManagementPlugin","text":"<p>ArgoCD supports ConfigManagementPlugins (CMP). These plugins allow you to extend the functionality of ArgoCD, e.g. by adding a new config management tool. In my case I use a CMP to encrypt helm values before kustomize is applied. See Secret Management for more information.</p>"},{"location":"argocd/#ignoring-values","title":"Ignoring values","text":"<p>The <code>ignoreDifferences</code> field is a powerful feature in Argo CD Application manifests that allows you to specify certain fields in your Kubernetes resources that should be ignored during ArgoCDs comparison between the live state and desired state defined in the Git repository. I use it to tell ArgoCD to e.g. ignore the number of replicas from a deployment or ignore the caBundle value from kube-prometheus-stack webhooks.</p>"},{"location":"argocd/#detailed-explanation","title":"Detailed Explanation:","text":"<pre><code>  ignoreDifferences:\n    - group: apps\n      kind: Deployment\n      name: emby\n      namespace: media\n      jsonPointers:\n        - /spec/replicas\n</code></pre> <ul> <li> <p>group: Specifies the API group of the resource</p> </li> <li> <p>kind: The type of Kubernetes resource to which this rule applies. Here, it is <code>Deployment</code>.</p> </li> <li> <p>name: The name of the specific resource to which the ignore rule applies, in this case, <code>emby</code>.</p> </li> <li> <p>namespace: The namespace in which the resource resides. This indicates where the <code>Deployment</code> object <code>emby</code> is located.</p> </li> <li> <p>jsonPointers: Lists specific JSON paths within the resource's manifest to ignore during synchronization. JSON Pointers are a way to reference specific parts of a JSON object. In this example, <code>- /spec/replicas</code> tells Argo CD to ignore differences in the <code>replicas</code> field of the <code>Deployment</code> specification.</p> </li> </ul>"},{"location":"argocd/#practical-implications","title":"Practical Implications:","text":"<p>Ignoring differences specified in the <code>jsonPointers</code> allows certain dynamic fields within Kubernetes resources to change (e.g., the number of replicas during autoscaling) without triggering an unwanted sync or causing the application to appear out of sync in Argo CD. This is particularly useful for configurations where you want Argo CD to overlook certain operational details that are managed externally or dynamically, rather than being strictly controlled through GitOps.</p>"},{"location":"auth/","title":"Authentication and Single Sign-On (SSO)","text":"<p>Overview</p> <p>This cluster implements a comprehensive authentication architecture with Single Sign-On capabilities using Authelia as the identity provider, LLDAP for user management, and Traefik ForwardAuth middleware to protect all exposed services.</p>"},{"location":"auth/#architecture","title":"Architecture","text":"<p>The authentication system consists of four main components working together:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          User Request                           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Traefik Ingress Controller                  \u2502\n\u2502                 (ForwardAuth Middleware Applied)                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                          Authelia                               \u2502\n\u2502                  (Authentication &amp; Authorization)               \u2502\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510           \u2502\n\u2502  \u2502   Session   \u2502    \u2502  Storage   \u2502    \u2502    OIDC     \u2502           \u2502\n\u2502  \u2502   (Redis)   \u2502    \u2502 (MariaDB)  \u2502    \u2502  Provider   \u2502           \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                 \u2502\n                                 v\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                            LLDAP                                \u2502\n\u2502                   (LDAP User Directory)                         \u2502\n\u2502                      (MariaDB Backend)                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"auth/#component-responsibilities","title":"Component Responsibilities","text":"Component Role Storage Backend Namespace Authelia Authentication, authorization, session management, OIDC provider MariaDB (storage), Redis (sessions) <code>authelia</code> LLDAP Lightweight LDAP directory for user/group management MariaDB <code>authelia</code> Redis Session storage for Authelia In-memory with optional persistence <code>databases</code> MariaDB Persistent storage for Authelia data and LLDAP users Persistent volumes <code>databases</code> Traefik ForwardAuth middleware enforcement N/A <code>kube-system</code>"},{"location":"auth/#deployment-architecture","title":"Deployment Architecture","text":""},{"location":"auth/#sync-wave-order","title":"Sync Wave Order","text":"<p>Applications are deployed in a specific order via ArgoCD sync waves to ensure dependencies are met:</p> <ol> <li> <p>Wave 09: Redis (Session backend)    - Helm chart: <code>bitnami/redis:23.1.1</code>    - Deployed to: <code>databases</code> namespace    - See: <code>apps/argo-cd-apps/09-redis.yaml</code></p> </li> <li> <p>Wave 10: Authelia (Authentication service)    - Helm chart: <code>authelia/authelia:0.10.46</code>    - Deployed to: <code>authelia</code> namespace    - Replicas: 2 (high availability)    - See: <code>apps/argo-cd-apps/10-authelia.yaml</code></p> </li> <li> <p>Wave 11: LLDAP (User directory)    - Custom deployment    - Image: <code>ghcr.io/lldap/lldap:latest-alpine</code>    - Deployed to: <code>authelia</code> namespace    - See: <code>apps/argo-cd-apps/11-lldap.yaml</code></p> </li> </ol> <p>MariaDB Dependency</p> <p>MariaDB is deployed earlier in sync wave 08 and must be available before Authelia and LLDAP start. See the Databases documentation for details.</p>"},{"location":"auth/#authelia-configuration","title":"Authelia Configuration","text":"<p>Authelia is responsible for authentication and authorization.</p>"},{"location":"auth/#authentication-backend-ldap","title":"Authentication Backend: LDAP","text":"<p>Authelia uses LLDAP as its authentication backend:</p> <pre><code>authentication_backend:\n  password_reset:\n    disable: false\n  ldap:\n    enabled: true\n    implementation: custom\n    address: ldaps://lldap.authelia.svc.cluster.local:6360\n    base_dn: DC=example,DC=com\n    additional_users_dn: ou=people\n    users_filter: (&amp;({username_attribute}={input})(objectClass=person))\n    additional_groups_dn: ou=groups\n    groups_filter: (member={dn})\n    attributes:\n      username: uid\n      group_name: cn\n      mail: mail\n    user: uid=admin,ou=people,dc=example,dc=com\n</code></pre> <p>Key Points: - Provices encrypted LDAP connection within cluster (cluster-internal communication) - Password reset functionality is enabled - Users are stored under <code>ou=people</code> - Groups are stored under <code>ou=groups</code> - Currently Authelia binds as the LDAP admin user to query directory</p>"},{"location":"auth/#session-management-redis","title":"Session Management: Redis","text":"<p>Sessions are stored in Redis for authelia scalability:</p> <pre><code>session:\n  name: authelia_session\n  expiration: 3600           # 1 hour\n  inactivity: 300            # 5 minutes\n  remember_me: 1 month\n  redis:\n    enabled: true\n    host: redis-master.databases.svc.cluster.local\n    port: 6379\n    database_index: 1\n</code></pre> <p>Session Behavior: - Sessions expire after 1 hour of total time - Sessions expire after 5 minutes of inactivity - \"Remember me\" option extends session to 1 month - Sessions are stored in Redis database index 1 (separate from other applications)</p>"},{"location":"auth/#storage-backend-mariadb","title":"Storage Backend: MariaDB","text":"<p>Authelia stores persistent data in MariaDB:</p> <pre><code>storage:\n  mysql:\n    enabled: true\n    address: tcp://mariadb.databases.svc.cluster.local:3306\n    database: authelia\n    username: authelia\n</code></pre> <p>Stored Data: - User preferences - TOTP secrets for 2FA - U2F device registrations - OAuth2 consent decisions - Identity verification tokens</p>"},{"location":"auth/#access-control-policies","title":"Access Control Policies","text":"<p>The default policy is <code>deny</code> with explicit rules granting access:</p> <pre><code>access_control:\n  default_policy: deny\n  rules:\n    - domain: '*.k8s.example.com'\n      policy: two_factor\n</code></pre> <p>Policy Explanation: - All access is denied by default - Any subdomain under <code>k8s.example.com</code> and <code>internal.example.com</code> requires two-factor authentication - Additional rules can be added for specific applications or paths</p> <p>Two-Factor Enforcement</p> <p>All exposed services require two-factor authentication (TOTP or U2F). Users must configure 2FA before accessing protected resources.</p>"},{"location":"auth/#oidc-provider","title":"OIDC Provider","text":"<p>Authelia acts as an OpenID Connect provider for applications that support native OIDC integration:</p> <pre><code>identity_providers:\n  oidc:\n    enabled: true\n    clients:\n      - client_id: grafana\n        client_name: Grafana\n        authorization_policy: two_factor\n        redirect_uris:\n          - https://grafana.example.com/login/generic_oauth\n        scopes:\n          - openid\n          - profile\n          - groups\n          - email\n\n      - client_id: argocd\n        client_name: Argo CD\n        authorization_policy: two_factor\n        redirect_uris:\n          - https://argocd.example.com/auth/callback\n        scopes:\n          - openid\n          - profile\n          - groups\n          - email\n</code></pre> <p>Some OIDC Clients: - Grafana: Integrated for monitoring dashboard access - Argo CD: Integrated for GitOps management console - Semaphore: Integrated for Ansible playbook automation</p> <p>Adding OIDC Clients</p> <p>To add a new OIDC client, generate a client secret using: <pre><code>docker run authelia/authelia:latest authelia crypto hash generate pbkdf2 \\\n  --variant sha512 --random --random.length 72 --random.charset rfc3986\n</code></pre> Then add the client configuration to <code>apps/authelia/values.enc.yaml</code>.</p>"},{"location":"auth/#notification-provider-smtp","title":"Notification Provider: SMTP","text":"<p>Authelia sends notifications via SMTP for password resets and identity verification:</p> <pre><code>notifier:\n  smtp:\n    enabled: true\n    address: smtp://192.168.1.45:25\n    sender: Authelia &lt;auth@example.com&gt;\n    disable_require_tls: true\n</code></pre> <p>Internal SMTP</p> <p>This configuration uses an SMTP relay outside of the cluster, but from within the home network, without authentication.</p>"},{"location":"auth/#authorization-endpoints","title":"Authorization Endpoints","text":"<p>Authelia exposes multiple authorization endpoints for different middleware types:</p> <pre><code>server:\n  endpoints:\n    authz:\n      forward-auth:\n        implementation: ForwardAuth\n        authn_strategies:\n          - name: HeaderAuthorization\n            schemes:\n              - Basic\n          - name: CookieSession\n      auth-request:\n        implementation: AuthRequest\n      ext-authz:\n        implementation: ExtAuthz\n</code></pre> <ul> <li>ForwardAuth: Used by Traefik (currently in use)</li> <li>AuthRequest: Compatible with NGINX ingress</li> <li>ExtAuthz: Compatible with Envoy proxy</li> </ul>"},{"location":"auth/#lldap-configuration","title":"LLDAP Configuration","text":"<p>LLDAP provides a lightweight, opinionated LDAP directory with a web UI.</p>"},{"location":"auth/#deployment","title":"Deployment","text":"<p>LLDAP runs as a single replica deployment:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lldap\n  namespace: authelia\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n        - name: lldap\n          image: ghcr.io/lldap/lldap:latest-alpine\n          ports:\n            - containerPort: 3890   # LDAP\n            - containerPort: 6360   # LDAPS\n            - containerPort: 17170  # Web UI\n</code></pre>"},{"location":"auth/#service-endpoints","title":"Service Endpoints","text":"<p>LLDAP exposes two service ports:</p> <ul> <li>Port 3890: LDAP protocol</li> <li>Port 6360: LDAPS protocol</li> <li>Port 17170: Web management interface</li> </ul>"},{"location":"auth/#web-interface-access","title":"Web Interface Access","text":"<p>LLDAP has its own ingress for administrative management:</p> <p>LLDAP Admin Access</p> <p>The LLDAP web interface is accessible but does NOT have ForwardAuth middleware applied. It uses its own internal authentication.</p>"},{"location":"auth/#user-and-group-management","title":"User and Group Management","text":"<p>LLDAP follows a simplified LDAP schema:</p> <ul> <li>Base DN: <code>DC=example,DC=com</code></li> <li>Users: <code>ou=people,dc=example,dc=com</code></li> <li>Groups: <code>ou=groups,dc=example,dc=com</code></li> </ul> <p>Adding Users: 1. Access the LLDAP web interface at the configured ingress 2. Navigate to \"Users\" and click \"Create User\" 3. Fill in required fields: username, email, display name 4. Set initial password 5. Assign groups as needed</p> <p>Managing Groups: 1. Navigate to \"Groups\" in LLDAP UI 2. Create groups to organize users 3. Add users to groups 4. Use groups in Authelia access control policies or OIDC claims</p>"},{"location":"auth/#traefik-integration","title":"Traefik Integration","text":"<p>Traefik enforces authentication using the ForwardAuth middleware.</p>"},{"location":"auth/#forwardauth-middleware","title":"ForwardAuth Middleware","text":"<p>The Authelia Helm chart automatically creates a ForwardAuth middleware resource:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: Middleware\nmetadata:\n  name: forwardauth-authelia\n  namespace: authelia\nspec:\n  forwardAuth:\n    address: http://authelia.authelia.svc.cluster.local/api/authz/forward-auth\n    trustForwardHeader: true\n    authResponseHeaders:\n      - Remote-User\n      - Remote-Groups\n      - Remote-Name\n      - Remote-Email\n</code></pre> <p>Middleware Behavior: - Traefik sends request headers to Authelia - Authelia validates authentication and authorization - If authenticated: Request proceeds with identity headers added - If not authenticated: User redirected to Authelia login page</p>"},{"location":"auth/#protecting-services-with-forwardauth","title":"Protecting Services with ForwardAuth","text":"<p>To protect an application with SSO, add the middleware annotation to its Ingress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: my-app\n  namespace: my-namespace\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: websecure\n    traefik.ingress.kubernetes.io/router.tls: \"true\"\n    traefik.ingress.kubernetes.io/router.middlewares: traefik-redirect@kubernetescrd, authelia-forwardauth-authelia@kubernetescrd\nspec:\n  ingressClassName: traefik\n  rules:\n    - host: my-app.k8s.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: my-app\n                port:\n                  number: 80\n</code></pre>"},{"location":"auth/#secret-management","title":"Secret Management","text":"<p>All sensitive authentication data is encrypted with SOPS.</p>"},{"location":"auth/#encrypted-files","title":"Encrypted Files","text":"<p>Authentication secrets are stored in:</p> <ul> <li><code>apps/authelia/values.enc.yaml</code>: Authelia secrets (LDAP password, session keys, OIDC secrets)</li> <li><code>apps/lldap/lldap-secret.enc.yaml</code>: LLDAP admin password and JWT secret</li> <li><code>apps/redis/values.enc.yaml</code>: Redis password (if authentication enabled; currently disabled)</li> </ul>"},{"location":"auth/#secret-rotation","title":"Secret Rotation","text":"<p>Rotating OIDC Client Secrets:</p> <ol> <li> <p>Generate new client secret: <pre><code>docker run authelia/authelia:latest authelia crypto hash generate pbkdf2 \\\n  --variant sha512 --random --random.length 72 --random.charset rfc3986\n</code></pre></p> </li> <li> <p>Update the application configuration with the new secret</p> </li> <li>Update <code>apps/authelia/values.enc.yaml</code> with the new hashed secret</li> <li>Commit changes and let ArgoCD sync</li> </ol> <p>Rotating Session Encryption Key:</p> <ol> <li>Generate new random key (64 characters recommended)</li> <li>Update <code>session.encryption_key.value</code> in Authelia configuration</li> <li>Commit and sync</li> <li>All existing sessions will be invalidated</li> </ol> <p>Session Key Rotation</p> <p>Rotating the session encryption key will log out all users.</p>"},{"location":"auth/#troubleshooting","title":"Troubleshooting","text":""},{"location":"auth/#emby-ldaps-connection","title":"emby ldaps connection","text":"<p>Emby requires the sha1 fingerprint from the certificate. After a renewal, the sha1 fingerprint changes and emby needs the new one. To get the fingerprint use the following command in a pod within the cluster (e.g. netshoot):</p> <pre><code>openssl s_client -showcerts -connect lldap.authelia.svc.cluster.local:6360 &lt;/dev/null 2&gt;/dev/null \\\n  | openssl x509 -noout -fingerprint -sha1 \\\n  | sed 's/://g' | sed 's/SHA1 Fingerprint=//'\nsha1 Fingerprint=77FE790909B81EDDDB8523DCD4206D7BA0B503B8\n</code></pre>"},{"location":"auth/#login-failures","title":"Login Failures","text":"<p>Symptom: User cannot log in with correct credentials</p> <p>Diagnosis: <pre><code># Check Authelia logs\nkubectl logs -n authelia -l app.kubernetes.io/name=authelia --tail=100\n\n# Check LLDAP logs\nkubectl logs -n authelia -l app.kubernetes.io/name=lldap --tail=100\n\n# Test LDAP connection from netshoot pod\nkubectl exec -n kube-system deployment/netshoot -- \\\n  apk add openldap-clients\n\nkubectl exec -n kube-system deployment/netshoot -- \\\n sh -c 'LDAPTLS_REQCERT=never ldapsearch -H ldaps://lldap.authelia.svc.cluster.local:6360 \\\n  -D \"uid=admin,ou=people,dc=example,dc=com\" \\\n  -w \"PASSWORD\" -b \"dc=example,dc=com\"'\n</code></pre></p> <p>Common Causes: - LDAP bind password incorrect - LLDAP service not running - User doesn't exist in LLDAP - User not in required group</p>"},{"location":"auth/#best-practices","title":"Best Practices","text":""},{"location":"auth/#security-recommendations","title":"Security Recommendations","text":"<ol> <li> <p>Enforce Two-Factor Authentication    - Set <code>policy: two_factor</code> for all production services    - Use hardware security keys (U2F) for administrator accounts    - Regularly audit user 2FA enrollment</p> </li> <li> <p>Rotate Secrets Regularly    - Rotate OIDC client secrets annually    - Rotate session encryption keys during maintenance windows    - Use strong random passwords for LDAP bind accounts</p> </li> <li> <p>Monitor Authentication Logs    - Set up alerts for repeated login failures    - Monitor Authelia logs for suspicious activity    - Track OIDC token issuance</p> </li> <li> <p>Network Segmentation    - Use Cilium network policies to restrict access to authentication components    - Only allow necessary egress to Authelia from application pods    - Isolate LLDAP to only Authelia access</p> </li> <li> <p>Backup Critical Data    - Include MariaDB <code>authelia</code> database in backup strategy    - Backup LLDAP database regularly    - Store SOPS encryption keys securely off-cluster</p> </li> </ol>"},{"location":"auth/#access-control-design","title":"Access Control Design","text":"<ol> <li> <p>Use Groups for Authorization    - Create LDAP groups for different access levels (admins, users, viewers)    - Reference groups in Authelia access control policies    - Pass groups as OIDC claims to applications</p> </li> <li> <p>Granular Policies    - Define specific access rules per application    - Use domain and path-based rules for fine-grained control    - Document policy decisions in comments</p> </li> <li> <p>Test Access Policies    - Test with non-privileged user accounts    - Verify group membership affects access    - Ensure deny-by-default behavior</p> </li> </ol>"},{"location":"auth/#additional-resources","title":"Additional Resources","text":"<ul> <li>Authelia Documentation</li> <li>LLDAP Documentation</li> <li>Traefik ForwardAuth Documentation</li> <li>Cilium Network Policy Guide</li> </ul>"},{"location":"auth/#related-documentation","title":"Related Documentation","text":"<ul> <li>Databases - MariaDB and Redis configuration</li> <li>Network - Traefik ingress and network architecture</li> <li>Secret Management - SOPS encryption details</li> <li>Monitoring - Observability for authentication components</li> </ul>"},{"location":"backup/","title":"Backup","text":"<p>For backups I need a solution that backups the files within a persistend volume. I've taken a look at velero and the included backup solution from Longhorn but wasn't able to get what I want. Longhorn seems to be doing block based backups and with Velero I wasn't able to create file backups. Althoug I do not exclude that I've missed something.</p> <p>At the moment I'm evaluating restic as a sidecar container. The sidecar container uses crontab to create backups in regular intervals. The container mounts the crontab config and backup script from configmaps and reads the environment variables from a secret, which configure the backup.</p> <p>Backups with restic provide:</p> <ul> <li>\u2705 ... encryption</li> <li>\u2705 ... compression</li> <li>\u2705 ... deduplication</li> <li>\u2705 ... retention policies</li> </ul> <p>So I can upload my backup to any storage backend without leaking any data. And because of deduplication and compression a lot of storage space can be saved. Included retention policies make it easy to delete old backups.</p> <p>Script Features:</p> <ul> <li>Backup any folder to a restic supported storage backend</li> <li>Delete old backups (Daily, Weekly, Monthly, Always Keep Last)</li> <li>ntfy.sh notification on failure</li> <li>prometheus pushgateway metrics</li> </ul> <p>Info</p> <p>The script runs restic and uploads the backup to the storage backend. Additionally it deletes old backups. It get's the configuration from environment variables.</p> Sidecar deployment example<pre><code>apiVersion: apps/v1\nkind: Deployment\nspec:\n  template:\n    spec:\n      containers:\n        - name: backup-sidecar\n          image: ghcr.io/restic/restic:0.17.1\n          envFrom:\n            - secretRef:\n                name: backup-env-configuration\n          imagePullPolicy: IfNotPresent\n          command: [\"/usr/sbin/crond\", \"-f\", \"-d\", \"6\"]\n          volumeMounts:\n            - mountPath: /backup/config\n              name: config\n              readOnly: true\n            - name: crontab-config\n              mountPath: /etc/crontabs/root\n              subPath: crontab\n            - name: backup-script\n              mountPath: /usr/local/bin/backup.sh\n              subPath: backup.sh\n      volumes:\n        - name: crontab-config\n          configMap:\n            name: crontab\n        - name: backup-script\n          configMap:\n            name: crontab-backup-script\n</code></pre> crontab configuration<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: crontab\ndata:\n  crontab: |\n    # m h  dom mon dow   command\n    # Run script every hour\n    0 * * * * /bin/sh /usr/local/bin/backup.sh\n</code></pre> restic configuration<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: backup-env-configuration\nstringData:\n  # Source directory to backup\n  RESTIC_SOURCE: /backup/config\n  # Backup repository (destination)\n  RESTIC_REPOSITORY: s3:s3.eu-central-2.wasabisys.com/path/to/backup\n  # Backup password\n  RESTIC_PASSWORD: &lt;BackupPassword&gt;\n  # Retention policies\n  KEEP_HOURLY: \"48\"\n  KEEP_DAILY: \"7\"\n  KEEP_WEEKLY: \"4\"\n  KEEP_LAST: \"1\"\n  # AWS credentials\n  AWS_ACCESS_KEY_ID: &lt;SecureSuperSecretKey&gt;\n  AWS_SECRET_ACCESS_KEY: &lt;VerySecureSuperSecretKey&gt;\n</code></pre> <p>I won't include the backup script here, because it can change over time. The script is available in my git repository as a ConfigMap. The ConfigMap get's replicated via reflector to all namespaces so all deployments can use the same script. But after changes to the script the pods using the script need to be restarted. This can be done by deleting the pods or by rolling out a new deployment.</p>"},{"location":"backup/#notifications","title":"Notifications","text":"<p>To get notified if a backup fails I'm using ntfy. Ntfy is a simple notification service that can be self-hosted.</p> <p></p>"},{"location":"backup/#pushgateway-integration","title":"Pushgateway Integration","text":"<p>Additionaly the backup script supports integration with a Prometheus Pushgateway to send custom metrics about the backup process. This enables tracking of backup duration, start time, and status.</p>"},{"location":"backup/#configuration","title":"Configuration","text":"<p>To enable metrics pushing to the Pushgateway, the following environment variables should be configured:</p> <ul> <li><code>PUSHGATEWAY_ENABLED</code>: Set this to <code>\"true\"</code> to enable sending metrics to the Pushgateway</li> <li><code>PUSHGATEWAY_URL</code>: Specify the URL of the Pushgateway server where metrics should be sent to</li> </ul>"},{"location":"backup/#metrics-published","title":"Metrics Published","text":"<p>Warning</p> <p>The metrics might change in the future. Currently I'm not realy satisfied. But maybe that's because I wasn't able to create a good grafana dashboard with them yet. I would appreciate any help. See issue #3</p> <p>The script publishes the following metrics to the Pushgateway:</p> <ul> <li><code>backup_duration_seconds</code>: The time, in seconds, that the backup process took</li> <li><code>backup_start_timestamp</code>: The timestamp in epoch at which the backup process began</li> <li><code>backup_status</code>: The status of the backup process, with either <code>status=\"success\"</code> or <code>status=\"failure\"</code></li> </ul> <p>Example Environment Configuration:</p> <pre><code>PUSHGATEWAY_ENABLED: \"true\"\nPUSHGATEWAY_URL: http://pushgateway.monitoring.svc.cluster.local\n</code></pre>"},{"location":"backup/#environment-variables","title":"\ud83d\udcdd Environment Variables","text":"<p>The following environment variables are used to configure the backup script.</p> Environment Variable Default Description <code>RESTIC_SOURCE</code> Unset Source directory to back up using Restic <code>RESTIC_REPOSITORY</code> Unset Destination repository for the backup <code>RESTIC_PASSWORD</code> Unset Password for encrypting the backup <code>RESTIC_HOSTNAME</code> <code>$(hostname | cut -d '-' -f1)</code> Optional. Hostname to use for the backup. Defaults to the pod name. Especially usefull for pods with host networking. <code>AWS_ACCESS_KEY_ID</code> Unset Access key ID for authenticating with an S3 compatible storage backend <code>AWS_SECRET_ACCESS_KEY</code> Unset Secret access key for authenticating with an S3 compatible storage backend <code>RESTIC_RETENTION_POLICIES_ENABLED</code> <code>true</code> Optional. Enable or disable retention policies <code>KEEP_HOURLY</code> 24 Optional. Number of hourly backups to retain <code>KEEP_DAILY</code> 7 Optional. Number of daily backups to keep <code>KEEP_WEEKLY</code> 4 Optional. Number of weekly backups to maintain <code>KEEP_MONTHLY</code> 12 Optional. Number of monthly backups to keep. Not implemented yet. <code>KEEP_YEARLY</code> 0 Optional. Number of yearly backups to keep. Not implemented yet. <code>KEEP_LAST</code> 1 Optional. Total number of most recent backups to keep, irrespective of time-based intervals <code>NTFY_ENABLED</code> false Optional. Indicates whether notification via ntfy is enabled. Possible values are <code>\"true\"</code> or <code>\"false\"</code> <code>NTFY_TITLE</code> <code>${RESTIC_HOSTNAME - Backup failed}</code> Optional. Title of the ntfy notification message. Can be a string or shell command <code>NTFY_CREDS</code> Unset Optional. Credentials for authenticating with the ntfy notification service. Needs to include the <code>-u</code> option <code>NTFY_PRIO</code> 4 Optional. Priority level for the ntfy notification. Determines the importance of the notification <code>NTFY_TAG</code> bangbang Optional. Tags to categorize the ntfy notification, allowing filtering or grouping of messages <code>NTFY_SERVER</code> ntfy.sh Optional. URL of the ntfy server used for sending notifications <code>NTFY_TOPIC</code> backup Optional. Topic on the ntfy server where the message will be sent to <code>PUSHGATEWAY_ENABLED</code> false Optional. Indicates whether sending metrics to the Pushgateway is enabled. Possible values are <code>\"true\"</code> or <code>\"false\"</code> <code>PUSHGATEWAY_URL</code> Unset Optional. URL of the Pushgateway server for sending metrics Example environment variables<pre><code>RESTIC_SOURCE: /backup/config\nRESTIC_REPOSITORY: s3:s3.eu-central-2.wasabisys.com/k3s-at-home-01/emby\nRESTIC_PASSWORD: bDuSsDS7uWf0OGrK4y5SBvEfIKkIVcK3gGZpxsVx6Ya6PfwkWANDZo8mRaoGnCE6\nKEEP_HOURLY: \"48\"\nKEEP_DAILY: \"7\"\nKEEP_WEEKLY: \"4\"\nKEEP_LAST: \"1\"\nAWS_ACCESS_KEY_ID: v1eoAeRYfHhcRsUsW\nAWS_SECRET_ACCESS_KEY: Hlk6wZiKdrqIafYLOdMbw9Z7WfKK8W6ata\nNTFY_ENABLED: \"true\"\nNTFY_TITLE: $(hostname | cut -d '-' -f1) - Backup failed\n# Needs to be with \"-u\"\nNTFY_CREDS: -u mne-adm:qhCVXJvzkf9SkjgFE9RDhtzycKbszdSnVw7fHFgS3cZCDmZMno25yfVhikrnPidS\nNTFY_PRIO: \"4\"\nNTFY_TAG: bangbang\nNTFY_SERVER: https://ntfy.geekbundle.org\nNTFY_TOPIC: kubernetes-at-home\nPUSHGATEWAY_ENABLED: \"true\"\nPUSHGATEWAY_URL: http://pushgateway.monitoring.svc.cluster.local:9091\n</code></pre>"},{"location":"backup/#rclone","title":"rclone","text":"<p>At some point I was also evaluating rclone as a sidecar container. But it doesn't support de-duplication and I want my storage costs to be as low as possible. For history reasons I keep the script that I've written and mounted in the container.</p> <p>rclone-backup.sh<pre><code>#!/bin/sh\n\n# Create run.log only if it does not exist\nif [ ! -f /data/run.log ]; then\n  touch /data/run.log\nfi\n\n# Echo current date to run.log for logging purposes\necho \"$(date) Backup process started\" &gt;&gt; /data/run.log\n\n# Sync source to destination with a backup directory using 'year-month-day' format\nbackup_dir=\"$BUP_DST/$(hostname | cut -d '-' -f1)-$(date +%Y-%m-%d)\"\nrclone --config /config/rclone/rclone.conf sync \"$BUP_SRC\" \"$BUP_DST/$(hostname | cut -d '-' -f 1)\" -v --backup-dir=\"$backup_dir\"\n\n# Calculate the date for retention time\nRETENTION_DATE=$(date -d '7 days ago' +%Y-%m-%d)\n\n# List directories and delete those older than retention time\nrclone --config /config/rclone/rclone.conf lsf \"$BUP_DST/\" --dirs-only | while read dir; do\n  # Extract the date from the directory name\n  dir_date=$(echo \"$dir\" | awk -F'-' '{print $NF}' | sed 's#/##g')\n  # Compare directory date with RETENTION_DATE\n  if [ \"$dir_date\" \\&lt; \"$RETENTION_DATE\" ]; then\n    echo \"Deleting old backup directory: $BUP_DST/$dir\" &gt;&gt; /data/run.log\n    rclone --config /config/rclone/rclone.conf purge \"$BUP_DST/$dir\"\n  fi\ndone\n\n# Log the completion of the backup process\necho \"$(date) Backup process completed\" &gt;&gt; /data/run.log\n</code></pre> This required the following config map mounted to /config/rclone/rclone.conf</p> rclone configuration<pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: rclone-config\n  namespace: whoami\nstringData:\n  rclone.conf: |\n    [WasabiFrankfurt]\n    type = s3\n    provider = Wasabi\n    env_auth = false\n    access_key_id = &lt;SuperSecretImportantPrivy&gt;\n    secret_access_key = &lt;VeryVerySuperSecretImportantPrivy&gt;\n    region = eu-central-2\n    endpoint = s3.eu-central-2.wasabisys.com\n    acl =\n</code></pre>"},{"location":"certmanager/","title":"Certificate Manager","text":"<p>cert-manager is a Kubernetes add-on that automates the management and issuance of TLS certificates from various issuing sources. In this cluster, it handles all TLS certificate provisioning for ingress resources using Let's Encrypt as the certificate authority with CloudFlare DNS-01 challenges.</p>"},{"location":"certmanager/#overview","title":"Overview","text":"<p>The cert-manager deployment in this cluster provides:</p> <ul> <li>Automated certificate issuance from Let's Encrypt (staging and production)</li> <li>DNS-01 challenge solving via CloudFlare API integration</li> <li>Wildcard certificate support</li> <li>Automatic certificate renewal before expiration</li> <li>Certificate distribution across namespaces via Reflector</li> <li>Integration with Traefik ingress controller</li> </ul>"},{"location":"certmanager/#deployment-details","title":"Deployment Details","text":""},{"location":"certmanager/#helm-chart-installation","title":"Helm Chart Installation","text":"<p>cert-manager is deployed via ArgoCD using the Kustomize Helm chart integration. The deployment is managed in <code>/apps/certmanager/</code> with sync wave priority <code>1</code>, making it one of the first applications to be deployed after core infrastructure.</p> <p>Version: v1.19.0 (Helm chart from https://charts.jetstack.io)</p> <p>ArgoCD Application: <code>apps/argo-cd-apps/01-certmanager.yaml</code></p> <p>Key Configuration Options:</p> <pre><code>helmCharts:\n  - name: cert-manager\n    repo: https://charts.jetstack.io\n    version: v1.19.0\n    releaseName: cert-manager\n    namespace: certmanager\n    valuesInline:\n      crds:\n        enabled: true\n        keep: true\n      dns01RecursiveNameserversOnly: true\n      dns01RecursiveNameservers: \"9.9.9.9:53,1.1.1.1:53\"\n      global:\n        revisionHistoryLimit: 3\n</code></pre>"},{"location":"certmanager/#dns-challenge-configuration","title":"DNS Challenge Configuration","text":"<p>The cluster uses DNS-01 challenges instead of HTTP-01 challenges. This approach has several advantages:</p> <ul> <li>Supports wildcard certificates</li> <li>Works behind firewalls or private networks</li> <li>No need to expose port 80 during validation</li> </ul> <p>The DNS challenge uses recursive nameservers (Quad9 and Cloudflare) to ensure reliable DNS propagation checking before attempting validation with Let's Encrypt.</p>"},{"location":"certmanager/#clusterissuers","title":"ClusterIssuers","text":"<p>Two ClusterIssuers are configured for certificate management:</p>"},{"location":"certmanager/#production-issuer","title":"Production Issuer","text":"<p>Name: <code>cloudflare-issuer-production</code></p> <p>ACME Server: <code>https://acme-v02.api.letsencrypt.org/directory</code></p> <p>This issuer is used for production certificates with Let's Encrypt's rate limits enforced. It is the default issuer for all production ingress resources.</p>"},{"location":"certmanager/#staging-issuer","title":"Staging Issuer","text":"<p>Name: <code>cloudflare-issuer-staging</code></p> <p>ACME Server: <code>https://acme-staging-v02.api.letsencrypt.org/directory</code></p> <p>This issuer should be used for testing certificate configuration before switching to production. The staging server has much higher rate limits and issues certificates from a test CA that browsers will not trust.</p>"},{"location":"certmanager/#clusterissuer-configuration","title":"ClusterIssuer Configuration","text":"<p>Both issuers are configured with the following key settings:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: cloudflare-issuer-production\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: &lt;encrypted-email&gt;\n    privateKeySecretRef:\n      name: cloudflare-le-secret\n    solvers:\n      - dns01:\n          cloudflare:\n            email: &lt;encrypted-email&gt;\n            apiTokenSecretRef:\n              name: cloudflare-api-token\n              key: api-token\n</code></pre> <p>The ClusterIssuer definitions are stored in encrypted form at <code>/apps/certmanager/cloudflare_issuer.enc.yaml</code>.</p>"},{"location":"certmanager/#cloudflare-dns-integration","title":"CloudFlare DNS Integration","text":"<p>cert-manager uses a CloudFlare API token to manipulate DNS records for DNS-01 challenge validation. The API token is stored as an encrypted Kubernetes secret in <code>/apps/certmanager/secrets.enc.yaml</code>.</p>"},{"location":"certmanager/#cloudflare-api-token-requirements","title":"CloudFlare API Token Requirements","text":"<p>The API token must have the following permissions:</p> <ul> <li>Zone: DNS:Edit for the zones you want to issue certificates for</li> <li>Zone: Zone:Read for the zones you want to issue certificates for</li> </ul>"},{"location":"certmanager/#secret-configuration","title":"Secret Configuration","text":"<p>The CloudFlare API token is stored in a Kubernetes secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: cloudflare-api-token\n  namespace: certmanager\ndata:\n  api-token: &lt;base64-encoded-token&gt;\n</code></pre> <p>This secret is referenced by both ClusterIssuers and must be available in the <code>certmanager</code> namespace before certificate issuance can succeed.</p>"},{"location":"certmanager/#wildcard-certificates","title":"Wildcard Certificates","text":"<p>The cluster uses pre-provisioned wildcard certificates that are automatically distributed to all namespaces using the Reflector operator. This approach has several advantages:</p> <ul> <li>Single certificate covers all subdomains</li> <li>Reduces Let's Encrypt rate limit consumption</li> <li>Certificates are available immediately in new namespaces</li> <li>Centralized certificate management</li> </ul>"},{"location":"certmanager/#wildcard-certificate-resources","title":"Wildcard Certificate Resources","text":"<p>Two wildcard certificates are configured in <code>/apps/certmanager/wildcard.enc.yaml</code>:</p> <ol> <li>wildcard-cloudflare-production-01: Covers the primary domain</li> <li>wildcard-cloudflare-production-02: Covers a secondary domain</li> </ol> <p>Example certificate configuration:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: wildcard-cloudflare-production-01\n  namespace: certmanager\nspec:\n  secretName: wildcard-cloudflare-production-01\n  privateKey:\n    algorithm: ECDSA\n    size: 256\n  issuerRef:\n    name: cloudflare-issuer-production\n    kind: ClusterIssuer\n  commonName: &lt;encrypted-domain&gt;\n  dnsNames:\n    - &lt;encrypted-wildcard-domain&gt;\n  secretTemplate:\n    annotations:\n      reflector.v1.k8s.emberstack.com/reflection-allowed: \"true\"\n      reflector.v1.k8s.emberstack.com/reflection-auto-enabled: \"true\"\n</code></pre>"},{"location":"certmanager/#certificate-distribution-via-reflector","title":"Certificate Distribution via Reflector","text":"<p>The <code>secretTemplate</code> section includes Reflector annotations that automatically distribute the certificate secret to all namespaces:</p> <ul> <li><code>reflector.v1.k8s.emberstack.com/reflection-allowed: \"true\"</code>: Enables the secret to be reflected</li> <li><code>reflector.v1.k8s.emberstack.com/reflection-auto-enabled: \"true\"</code>: Automatically creates copies in all namespaces</li> </ul> <p>This means that once a wildcard certificate is issued in the <code>certmanager</code> namespace, it is immediately available in all other namespaces without manual intervention.</p>"},{"location":"certmanager/#using-certificates-in-ingress-resources","title":"Using Certificates in Ingress Resources","text":""},{"location":"certmanager/#standard-ingress-with-wildcard-certificate","title":"Standard Ingress with Wildcard Certificate","text":"<p>Most ingress resources in this cluster use the pre-provisioned wildcard certificates. Here's an example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: lldap\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web239,websecure239\n    traefik.ingress.kubernetes.io/router.tls: \"true\"\nspec:\n  ingressClassName: traefik\n  tls:\n    - hosts:\n        - lldap.example.com\n      secretName: wildcard-cloudflare-production-02\n  rules:\n    - host: lldap.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: lldap\n                port:\n                  number: 17170\n</code></pre> <p>Key points:</p> <ul> <li>The <code>tls</code> section references one of the wildcard certificate secrets</li> <li>The secret is automatically available in the namespace via Reflector</li> <li>No cert-manager annotations are needed when using pre-provisioned certificates</li> <li>The hostname must match the wildcard pattern</li> </ul>"},{"location":"certmanager/#requesting-application-specific-certificates","title":"Requesting Application-Specific Certificates","text":"<p>If you need a certificate for a specific application that is not covered by the wildcard certificates, you can create a Certificate resource:</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: myapp-tls\n  namespace: myapp\nspec:\n  secretName: myapp-tls\n  issuerRef:\n    name: cloudflare-issuer-production\n    kind: ClusterIssuer\n  dnsNames:\n    - myapp.example.com\n    - www.myapp.example.com\n</code></pre> <p>Then reference it in your Ingress:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myapp\nspec:\n  tls:\n    - hosts:\n        - myapp.example.com\n        - www.myapp.example.com\n      secretName: myapp-tls\n  rules:\n    - host: myapp.example.com\n      # ... rest of ingress configuration\n</code></pre>"},{"location":"certmanager/#certificate-renewal","title":"Certificate Renewal","text":"<p>cert-manager automatically renews certificates before they expire. The default renewal window is 30 days before expiration (for Let's Encrypt's 90-day certificates, renewal happens around day 60).</p>"},{"location":"certmanager/#monitoring-certificate-expiration","title":"Monitoring Certificate Expiration","text":"<p>You can check certificate status using kubectl:</p> <pre><code># View all certificates\nkubectl get certificates -n certmanager\n\n# Check certificate details\nkubectl describe certificate wildcard-cloudflare-production-01 -n certmanager\n\n# View certificate renewal status\nkubectl get certificaterequest -n certmanager\n</code></pre>"},{"location":"certmanager/#certificate-renewal-process","title":"Certificate Renewal Process","text":"<ol> <li>cert-manager monitors certificate expiration dates</li> <li>When a certificate enters the renewal window, cert-manager creates a CertificateRequest</li> <li>The ClusterIssuer creates a DNS challenge record via CloudFlare API</li> <li>Let's Encrypt validates the DNS record</li> <li>cert-manager retrieves the new certificate and updates the Secret</li> <li>Reflector automatically distributes the updated certificate to all namespaces</li> </ol>"},{"location":"certmanager/#troubleshooting","title":"Troubleshooting","text":""},{"location":"certmanager/#checking-cert-manager-logs","title":"Checking cert-manager Logs","text":"<pre><code># View cert-manager controller logs\nkubectl logs -n certmanager -l app=cert-manager -f\n\n# View webhook logs\nkubectl logs -n certmanager -l app=webhook -f\n\n# View cainjector logs\nkubectl logs -n certmanager -l app=cainjector -f\n</code></pre>"},{"location":"certmanager/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"certmanager/#certificate-not-issuing","title":"Certificate Not Issuing","text":"<p>Symptoms: Certificate remains in \"Pending\" state</p> <p>Troubleshooting steps:</p> <ol> <li> <p>Check the Certificate status:    <pre><code>kubectl describe certificate &lt;certificate-name&gt; -n certmanager\n</code></pre></p> </li> <li> <p>Check for CertificateRequest objects:    <pre><code>kubectl get certificaterequest -n certmanager\nkubectl describe certificaterequest &lt;request-name&gt; -n certmanager\n</code></pre></p> </li> <li> <p>Check the Order and Challenge status:    <pre><code>kubectl get orders -n certmanager\nkubectl describe order &lt;order-name&gt; -n certmanager\n\nkubectl get challenges -n certmanager\nkubectl describe challenge &lt;challenge-name&gt; -n certmanager\n</code></pre></p> </li> <li> <p>Verify CloudFlare API token permissions and validity</p> </li> </ol>"},{"location":"certmanager/#dns-challenge-failing","title":"DNS Challenge Failing","text":"<p>Symptoms: Challenge remains in \"Pending\" state, events show DNS validation failures</p> <p>Troubleshooting steps:</p> <ol> <li> <p>Verify the CloudFlare API token secret exists:    <pre><code>kubectl get secret cloudflare-api-token -n certmanager\n</code></pre></p> </li> <li> <p>Check if DNS records are being created in CloudFlare:    - Log into CloudFlare dashboard    - Navigate to DNS records for your domain    - Look for <code>_acme-challenge</code> TXT records</p> </li> <li> <p>Verify DNS propagation:    <pre><code>dig _acme-challenge.example.com TXT @9.9.9.9\ndig _acme-challenge.example.com TXT @1.1.1.1\n</code></pre></p> </li> <li> <p>Check cert-manager can reach DNS servers:    <pre><code>kubectl logs -n certmanager -l app=cert-manager | grep -i dns\n</code></pre></p> </li> </ol>"},{"location":"certmanager/#certificate-not-distributing-to-namespaces","title":"Certificate Not Distributing to Namespaces","text":"<p>Symptoms: Certificate exists in <code>certmanager</code> namespace but not in other namespaces</p> <p>Troubleshooting steps:</p> <ol> <li> <p>Verify Reflector is running:    <pre><code>kubectl get pods -n reflector\n</code></pre></p> </li> <li> <p>Check Reflector annotations on the certificate secret:    <pre><code>kubectl get secret wildcard-cloudflare-production-01 -n certmanager -o yaml | grep -A5 annotations\n</code></pre></p> </li> <li> <p>Check Reflector logs:    <pre><code>kubectl logs -n reflector -l app.kubernetes.io/name=reflector\n</code></pre></p> </li> <li> <p>Verify the certificate secret exists in the source namespace:    <pre><code>kubectl get secret wildcard-cloudflare-production-01 -n certmanager\n</code></pre></p> </li> </ol>"},{"location":"certmanager/#rate-limiting","title":"Rate Limiting","text":"<p>Symptoms: Certificate issuance fails with rate limit errors</p> <p>Solutions:</p> <ol> <li> <p>Use the staging issuer for testing:    - Change <code>issuerRef.name</code> to <code>cloudflare-issuer-staging</code>    - Test your configuration thoroughly    - Switch back to production issuer once verified</p> </li> <li> <p>Let's Encrypt rate limits:    - 50 certificates per registered domain per week    - 5 duplicate certificates per week    - Use wildcard certificates to reduce certificate count</p> </li> <li> <p>Wait for rate limit window to reset (usually 7 days)</p> </li> </ol>"},{"location":"certmanager/#extracting-certificate-for-external-use","title":"Extracting Certificate for External Use","text":"<p>If you need to extract a certificate and private key for use outside Kubernetes:</p> <pre><code># Extract certificate\nkubectl get secret -n certmanager wildcard-cloudflare-production-01 \\\n  -o jsonpath=\"{.data['tls\\.crt']}\" | base64 -d &gt; certificate.crt\n\n# Extract private key (handle with care!)\nkubectl get secret -n certmanager wildcard-cloudflare-production-01 \\\n  -o jsonpath=\"{.data['tls\\.key']}\" | base64 -d &gt; private.key\n\n# Extract CA certificate if present\nkubectl get secret -n certmanager wildcard-cloudflare-production-01 \\\n  -o jsonpath=\"{.data['ca\\.crt']}\" | base64 -d &gt; ca.crt\n</code></pre> <p>Security note: Handle private keys with extreme care. Never commit them to version control or share them insecurely.</p>"},{"location":"certmanager/#manual-certificate-renewal","title":"Manual Certificate Renewal","text":"<p>To force certificate renewal (e.g., after fixing an issue):</p> <pre><code># Delete the secret to trigger re-issuance\nkubectl delete secret wildcard-cloudflare-production-01 -n certmanager\n\n# Or use cmctl to renew\ncmctl renew wildcard-cloudflare-production-01 -n certmanager\n</code></pre>"},{"location":"certmanager/#integration-with-other-components","title":"Integration with Other Components","text":""},{"location":"certmanager/#traefik-ingress-controller","title":"Traefik Ingress Controller","text":"<p>All ingress resources in this cluster use Traefik as the ingress controller. Traefik automatically reads certificate secrets referenced in the <code>tls</code> section of Ingress resources.</p> <p>Example Traefik ingress with TLS:</p> <pre><code>metadata:\n  annotations:\n    traefik.ingress.kubernetes.io/router.entrypoints: web239,websecure239\n    traefik.ingress.kubernetes.io/router.tls: \"true\"\nspec:\n  tls:\n    - hosts:\n        - app.example.com\n      secretName: wildcard-cloudflare-production-02\n</code></pre>"},{"location":"certmanager/#secret-management-with-sops","title":"Secret Management with SOPS","text":"<p>All cert-manager configuration files containing sensitive data are encrypted using SOPS with age encryption:</p> <ul> <li><code>/apps/certmanager/cloudflare_issuer.enc.yaml</code>: ClusterIssuer definitions with encrypted email addresses</li> <li><code>/apps/certmanager/wildcard.enc.yaml</code>: Certificate resources with encrypted domain names</li> <li><code>/apps/certmanager/secrets.enc.yaml</code>: CloudFlare API token secret</li> </ul> <p>To decrypt and edit these files:</p> <pre><code># Decrypt and edit in one command\nsops apps/certmanager/cloudflare_issuer.enc.yaml\n\n# Or decrypt to a separate file\nsops -d apps/certmanager/secrets.enc.yaml &gt; secrets-decrypted.yaml\n</code></pre> <p>For more information on secret management, see Secret Management.</p>"},{"location":"certmanager/#testing-certificate-configuration-locally","title":"Testing Certificate Configuration Locally","text":"<p>Before committing changes to certificate configuration, test them locally:</p> <pre><code># Build kustomization to preview resources\nkubectl kustomize apps/certmanager --enable-helm\n\n# Dry-run apply to validate syntax\nkubectl apply --dry-run=client -k apps/certmanager\n\n# Apply to test environment (if using Vagrant)\nexport KUBECONFIG=\"$PWD/shared/k3svm1/k3s.yaml\"\nkubectl kustomize apps/certmanager --enable-helm | kubectl apply -f -\n\n# Check ArgoCD sync diff\nargocd app diff 01-certmanager\n</code></pre>"},{"location":"certmanager/#best-practices","title":"Best Practices","text":"<ol> <li>Use wildcard certificates: Reduces certificate count and simplifies management</li> <li>Test with staging issuer: Always test new configurations with the staging issuer before production</li> <li>Monitor certificate expiration: Set up alerts for certificates nearing expiration (built into kube-prometheus-stack)</li> <li>Secure API tokens: Ensure CloudFlare API tokens have minimal required permissions</li> <li>Encrypt sensitive data: Use SOPS to encrypt all configuration containing domain names, emails, or tokens</li> <li>Document custom certificates: If creating application-specific certificates, document the reason for not using wildcards</li> <li>Regular secret rotation: Rotate CloudFlare API tokens periodically (recommended annually)</li> </ol>"},{"location":"certmanager/#related-documentation","title":"Related Documentation","text":"<ul> <li>Secret Management: SOPS encryption and decryption workflows</li> <li>Reflector: Certificate distribution across namespaces</li> <li>Network Architecture: Traefik ingress controller configuration</li> <li>ArgoCD: Application deployment and sync waves</li> <li>Monitoring: Certificate expiration alerts</li> </ul>"},{"location":"certmanager/#references","title":"References","text":"<ul> <li>cert-manager documentation</li> <li>Let's Encrypt rate limits</li> <li>CloudFlare API token documentation</li> <li>DNS-01 challenge documentation</li> </ul>"},{"location":"databases/","title":"Datenbanken","text":""},{"location":"databases/#mariadb","title":"MariaDB","text":"<p>Execute these commands in the shell of the MariaDB Container.</p> <p>Logging into the database:</p> <pre><code>mariadb -u root --password=$(cat $MARIADB_ROOT_PASSWORD_FILE)\n</code></pre> <p>Show all databases:</p> <pre><code>SHOW databases;\n</code></pre> <p>Show permiissions for user:</p> <pre><code>SHOW GRANTS FOR 'authelia'@'host';\n</code></pre> <p>Create database:</p> <pre><code>CREATE DATABASE authelia;\n</code></pre> <p>Create user with password:</p> <pre><code>CREATE USER 'authelia'@'%' IDENTIFIED BY '&lt;NotSoSecure&gt;';\nGRANT ALL PRIVILEGES ON authelia.* TO 'authelia'@'%';\nFLUSH PRIVILEGES;\n</code></pre> <p>Create user without password, but only allowed from specific networks:</p> <pre><code>CREATE USER 'authelia'@'10.42.0.0/255.255.0.0';\nGRANT ALL PRIVILEGES ON authelia.* TO 'authelia'@'10.42.0.0/255.255.0.0';\nFLUSH PRIVILEGES;\n</code></pre> <p>Delete user:</p> <pre><code>DROP USER 'authelia'@'%';\n</code></pre>"},{"location":"databases/#backup","title":"Backup","text":"<p>The backup for MariaDB in this setup is handled using <code>mariadb-backup</code> and <code>restic</code>. mariadb-backup creates a physical backup and restic stores the file in a repository, providing compression and deduplication.</p> Backup command<pre><code>mariadb-backup --host mariadb.databases.svc.cluster.local --user=root --password=$MARIADB_ROOT_PASSWORD --backup --target-dir=/backup --stream=xbstream &gt; /backup/mariadb.xb\n</code></pre>"},{"location":"databases/#restore-process","title":"Restore Process","text":"<p>The restoration of backups involves retrieving backup snapshots from restic, deserializing, and preparing for database usage.</p> <ol> <li>List Backups: Get available backups stored in the restic repository</li> </ol> <pre><code>restic snapshots --tag MariaDB\n</code></pre> <ol> <li>Restore Snapshot: Extract the latest snapshot to a target directory</li> </ol> <pre><code>restic restore latest --tag MariaDB --target .\n</code></pre> <ol> <li>Unserialize the Backup: Use <code>mbstream</code> to deserialize the backup file</li> </ol> <pre><code>mkdir mariadb_recovery\nmbstream -x &lt;mariadb.xb\n</code></pre> <ol> <li>Prepare the Recovery: Prepare the backup for use. If possible, use the same mariadb-backup version with which the backup was created</li> </ol> <pre><code>mariadb-backup --prepare --target-dir=./mariadb_recovery\n</code></pre> <p>More information about the process in the following fosdem presentation: mariabackup restic</p>"},{"location":"databases/#kubernetes-cronjob-for-automated-backups","title":"Kubernetes CronJob for Automated Backups","text":"<p>A Kubernetes CronJob is used to automate the MariaDB backups. At first it creates the backup with mariadb-backup and then stores it in a restic repository. To ensure that the processes run sequential and not parallel, the backup creation runs as init container and afterwards restic as normal container. For best compatibility the mariadb-backup command must be the same version as the MariaDB server. So the init Container of the cronjob executes the mariadb-backup binary within the mariadb container. This way I always get the correct mariadb-backup version.</p> <p>Explanation:</p> <ul> <li> <p>Service Account and Role: A service account <code>mariadb-backup-serviceaccount</code> is used, bound with a role <code>mariadb-backup-role</code> that has necessary permissions to get the pod name and exec into the container</p> </li> <li> <p>CronJob Configuration: The CronJob is set to run every hour, except 2 o'clock in the night <code>\"0 0-1,3-23 * * *\"</code></p> </li> <li> <p>Backup Initialization: <code>mariadb-backup</code> runs as an <code>initContainer</code> to ensure backups are taken before any other process begins</p> </li> <li> <p>Restic: <code>restic</code> stores the backup in a restic repository which is accessible via an s3 bucket</p> </li> <li> <p>Storage: A <code>PersistentVolumeClaim</code> (<code>longhorn-pvc-mariadb-backupvolume</code>) is used to store the current backup temporarily</p> </li> </ul> <p>The backup strategy is designed to ensure that <code>mariadb-backup</code> uses a version compatible with the running MariaDB server by executing it within the MariaDB container.</p> Shortened Kubernetes CronJob<pre><code>apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mariadb-backup\n  namespace: databases\nspec:\n  schedule: \"0 0-1,3-23 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: mariadb-backup-serviceaccount\n          initContainers:\n            - name: kubectl-mariadb-backup\n              image: bitnami/kubectl:latest\n              command:\n                - /bin/sh\n                - -c\n                - &gt;\n                  POD_NAME=$(kubectl get pods -n databases -l app.kubernetes.io/name=mariadb -o jsonpath=\"{.items[0].metadata.name}\") &amp;&amp;\n                  kubectl exec -n databases $POD_NAME -- mariadb-backup --host 127.0.0.1 --user=root --password=$MARIADB_ROOT_PASSWORD --backup --stream=xbstream &gt; /backup/mariadb.xb\n          containers:\n            - name: restic\n              image: ghcr.io/restic/restic:0.17.3\n              command:\n                - /bin/sh\n                - /usr/local/bin/backup.sh\n              volumeMounts:\n                - name: backup\n                  mountPath: /backup\n</code></pre>"},{"location":"databases/#redis","title":"Redis","text":"Database Application 1 authelia 2 paperless <p>Get all keys from a database:</p> <pre><code>redis-cli -n 1 KEYS '*'\n</code></pre> <pre><code>redis-cli SET server:name \"redis server\"\nredis-cli GET server:name\n</code></pre>"},{"location":"helm/","title":"Helm","text":"<p>Show possible helm chart settings:</p> <pre><code>helm show values prometheus-community/kube-prometheus-stack | less\n</code></pre> <p>Show installed settings:</p> <pre><code>helm list --all-namespaces\nhelm get values kube-prometheus-stack -n k3s-monitoring | less\n</code></pre> <p>Show all charts of a repository:</p> <pre><code>helm search repo prometheus-community\n</code></pre> <p>Show all repositories:</p> <pre><code>helm repo list\n</code></pre>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#requirements","title":"Requirements","text":"<ul> <li>git</li> <li>ansible</li> <li>kubectl</li> <li>helm</li> </ul>"},{"location":"install/#preparation","title":"Preparation","text":"<ul> <li>Clone this repository</li> </ul> <pre><code>git clone https://github.com/madic-creates/k3s-git-ops\n</code></pre> <ul> <li>Configure pre-commit-hooks</li> <li>Adapt ansible vars to your needs. The default variables are located in <code>ansible/group_vars/all/main.yaml</code>. You can override them by creating environment-specific variable files in <code>ansible/group_vars/production/</code> or <code>ansible/group_vars/staging/</code>.</li> </ul> <p>For example, to set the VIP for your production environment, create <code>ansible/group_vars/production/main.yaml</code> with the following content: <pre><code>k3s_vip: \"192.168.1.230\"\n</code></pre></p> <ul> <li>Install required ansible modules</li> </ul> <pre><code>ansible-galaxy collection install -r requirements.yaml\n</code></pre>"},{"location":"install/#ansible-inventory","title":"Ansible inventory","text":"<p>The inventory is now structured by environment (e.g., <code>production</code>, <code>staging</code>) inside the <code>ansible/inventory/</code> directory. This playbook requires the following host groups:</p> <ul> <li><code>k3s_primary_server</code>: The first server node that initializes the cluster.</li> <li><code>k3s_secondary_server</code>: Additional server nodes to join the cluster for HA.</li> <li><code>k3s_agent</code>: The agent/worker nodes.</li> </ul> <p>Example for <code>ansible/inventory/production/hosts</code>:</p> <pre><code>[k3s_primary_server]\nnode01.example.com\n\n[k3s_secondary_server]\nnode02.example.com\nnode03.example.com\n\n[k3s_agent]\nagent01.example.com\n</code></pre>"},{"location":"install/#installation_1","title":"Installation","text":"<p>Run ansible from within the ansible folder. Specify the inventory for your target environment.</p> <pre><code>cd ansible\nansible-playbook playbooks/install.yaml -i inventory/production/hosts --diff\n</code></pre> <p>Ansible downloads the kubeconfig file to the folder ./shared/${HOSTNAME}/k3s.yaml file. You can use this file to access the cluster.</p> <p>To get the validity of the kubeconfig file, you can use the following command (either replace $KUBECONFIG with the path to the kubeconfig file or set it as environment variable):</p> <pre><code>awk '/client-certificate-data:/ {print $2}' $KUBECONFIG | base64 -d | \\\nopenssl x509 -noout -startdate -enddate &amp;&amp; \\\necho \"Valid for: $(awk '/client-certificate-data:/ {print $2}' $KUBECONFIG | \\\nbase64 -d | openssl x509 -noout -enddate | cut -d= -f2 | \\\nxargs -I{} date -d \"{}\" +\"%s\" | \\\nawk -v now=$(date +%s) '{printf \"%.0f\\n\", ($1-now)/86400}') days\"\n</code></pre> <p></p>"},{"location":"install/#removal","title":"Removal","text":"<pre><code>cd ansible\nansible-playbook playbooks/remove.yaml -i inventory/production/hosts -l node01.example.com -K --diff\n</code></pre>"},{"location":"install/#argocd","title":"ArgoCD","text":"<pre><code>kubectl kustomize apps/argo-cd --enable-helm | kubectl apply -f -\nkubectl kustomize apps/overlays/vagrant | kubectl apply -f -\n</code></pre> <p>Get the ArgoCD Login password for the admin user:</p> <pre><code>kubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre> <p>Remove ArgoCD</p> <pre><code>kubectl kustomize bootstrap --enable-helm | kubectl delete -f -\n</code></pre>"},{"location":"kubedoom/","title":"Kube DOOM: Kubernetes Chaos Engineering, 1993 Style","text":"<p>DOOM: The DevOps Spin-Off Nobody Asked For</p> <p>In 1993, id Software asked \"What if we could kill demons with a shotgun?\" In 2025, some absolute legend asked \"What if we could kill pods with a shotgun?\" And thus, Kube DOOM was born.</p>"},{"location":"kubedoom/#what-fresh-hell-is-this","title":"What Fresh Hell Is This?","text":"<p>Kube DOOM is a Kubernetes cluster management tool that transforms pod termination into a first-person shooter experience. Because apparently <code>kubectl delete pod</code> wasn't visceral enough.</p> <p>Each pinky daemon in the game represents a pod in your cluster. When you frag a demon, the corresponding pod gets terminated. It's like chaos engineering, but with more pixelated blood and the iconic sound of a double-barreled shotgun.</p> <p>Currently configured to target the <code>whoami</code> namespace, because apparently we need to answer life's fundamental question: \"Who am I?\" with \"Someone who kills pods for fun.\"</p> <p>Therapeutic Pod Management</p> <p>Studies show that 87% of SREs experience immediate stress relief when replacing <code>kubectl delete</code> with a BFG9000. The other 13% are lying.</p>"},{"location":"kubedoom/#why-does-this-exist","title":"Why Does This Exist?","text":""},{"location":"kubedoom/#the-philosophical-answer","title":"The Philosophical Answer","text":"<p>Chaos engineering meets nostalgia. Resilience testing meets dopamine release. Your cluster meets the Doom Slayer.</p>"},{"location":"kubedoom/#the-practical-answer","title":"The Practical Answer","text":"<ul> <li>Visual Pod Management: Sometimes you just want to see what you're killing</li> <li>Stress Relief: 3 AM incident? Kill some pods with style</li> <li>Team Building: Nothing says \"DevOps culture\" like a multiplayer deathmatch against your own infrastructure</li> <li>Training: Teach juniors about pod lifecycle management through the universal language of violence</li> </ul>"},{"location":"kubedoom/#the-honest-answer","title":"The Honest Answer","text":"<p>Because we can. Because it's hilarious. Because the cluster can handle it (hopefully).</p> <p>Production Use Warning</p> <p>Should you run Kube DOOM in production? Technically, yes. Should you tell your manager? Absolutely not. Should you livestream it? Only if you've updated your LinkedIn profile recently.</p>"},{"location":"kubedoom/#installation-status","title":"Installation Status","text":"<p>Kube DOOM is deployed to the <code>kubedoom</code> namespace with sync wave <code>86</code> (which, fun fact, is also the number of times you'll reload your shotgun before finding that one crashlooping pod).</p> <p>Target Namespace: <code>whoami</code></p> <p>ArgoCD Application: <code>86-kube-doom.yaml</code></p>"},{"location":"kubedoom/#getting-started-your-journey-into-pod-purgatory","title":"Getting Started: Your Journey Into Pod Purgatory","text":""},{"location":"kubedoom/#prerequisites","title":"Prerequisites","text":"<ul> <li>VNC client (TigerVNC recommended, RealVNC if you're fancy)</li> <li>A cluster with pods you don't mind killing</li> <li>A sense of humor</li> <li>Questionable decision-making skills</li> </ul>"},{"location":"kubedoom/#step-1-establish-port-forward","title":"Step 1: Establish Port Forward","text":"<p>First, tunnel into the hellscape:</p> <pre><code>kubectl port-forward -n kubedoom service/kubedoom 5900:5900\n</code></pre> <p>Port Forwarding: The Gateway to Pod Hell</p> <p>Port 5900 is the standard VNC port. It's also coincidentally the number of times you've thought \"there has to be a better way\" before discovering this tool.</p>"},{"location":"kubedoom/#step-2-connect-via-vnc","title":"Step 2: Connect via VNC","text":"<p>Launch your VNC client and connect to:</p> <pre><code>127.0.0.1:5900\n</code></pre> <p>VNC Password: <code>idbehold</code></p> <p>Authentication Comedy</p> <p>The password is literally a DOOM cheat code. This is either brilliant security through obscurity or a cry for help from the developer.</p>"},{"location":"kubedoom/#step-3-rip-and-tear-pods","title":"Step 3: Rip and Tear (Pods)","text":"<ol> <li>Use arrow keys or WASD to navigate</li> <li>Spacebar to open doors (because even in pod management, some things require manual intervention)</li> <li>Ctrl/Left-click to fire your weapon of choice</li> <li>Kill demons to terminate pods</li> <li>Watch in horror/delight as pods respawn (if controlled by a Deployment)</li> <li>Question your career choices</li> <li>Realize this is the most fun you've had at work in months</li> </ol>"},{"location":"kubedoom/#cheat-codes-because-even-pod-management-needs-easy-mode","title":"Cheat Codes: Because Even Pod Management Needs Easy Mode","text":"<p>The classic DOOM cheats work in Kube DOOM, because why not add invincibility to cluster administration?</p> Cheat Code Effect DevOps Translation <code>IDDQD</code> God mode (invincibility) You've achieved SRE enlightenment - nothing can page you now <code>IDKFA</code> All weapons and ammo Full access to the Kubernetes API - use responsibly (narrator: they won't) <code>IDSPISPOPD</code> No-clip mode (walk through walls) Network policies? Never heard of them <code>IDCLEV</code> Level select Jump to different pod densities - try <code>IDCLEV31</code> for nightmare mode <code>IDDT</code> Full automap Like <code>kubectl get pods --all-namespaces</code> but more dramatic <code>IDCHOPPERS</code> Chainsaw For those intimate, close-range pod terminations <code>IDCLIP</code> No-clip mode (alternative) RBAC is just a suggestion <p>With Great Cheats Comes Great Responsibility</p> <p>Using <code>IDKFA</code> in production is like using <code>sudo rm -rf /</code> - you can do it, but your future self will have opinions.</p>"},{"location":"kubedoom/#activation-instructions","title":"Activation Instructions","text":"<p>To enter cheat codes: 1. Simply type them during gameplay (no console needed) 2. Screen will flash briefly to confirm activation 3. Your conscience will flash warnings about production safety 4. Ignore both and proceed with extreme prejudice</p>"},{"location":"kubedoom/#technical-details-the-boring-but-necessary-part","title":"Technical Details (The Boring But Necessary Part)","text":""},{"location":"kubedoom/#how-it-works","title":"How It Works","text":"<ol> <li>Kube DOOM watches the target namespace (<code>whoami</code>) for pod events</li> <li>Each pod is represented as a pinky daemon in the game</li> <li>When you kill a monster, Kube DOOM executes <code>kubectl delete pod</code></li> <li>Pod controllers (Deployments, StatefulSets) will respawn pods - because true chaos never dies</li> <li>You reload, respawn, and repeat</li> </ol>"},{"location":"kubedoom/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 VNC Client  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502  Kube DOOM   \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Whoami    \u2502\n\u2502  (You)      \u2502  5900   \u2502   Service    \u2502  K8s API\u2502  Namespace  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n      \u2502                        \u2502                         \u2502\n      \u2502                        \u2502                         \u2502\n      \u2514\u2500\u2500\u2500\u2500\u2500\u2500[PEW PEW]\u2500\u2500\u2500-\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500[kubectl delete]\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"kubedoom/#resource-configuration","title":"Resource Configuration","text":"<p>Kube DOOM is deployed via the upstream manifest from the official repository.</p> <p>Namespace: <code>kubedoom</code> - quarantined for everyone's safety</p> <p>Sync Wave: <code>86</code> - deployed late enough that you have pods to kill</p>"},{"location":"kubedoom/#troubleshooting-when-things-go-to-hellspawn","title":"Troubleshooting: When Things Go to Hell(spawn)","text":""},{"location":"kubedoom/#problem-ive-been-playing-for-4-hours","title":"Problem: I've Been Playing for 4 Hours","text":"<p>Symptoms: Complete loss of time awareness, questioning life choices</p> <p>Solutions: - This is working as intended - Take a break - Touch grass - Remember you have actual production issues to fix - Return in 10 minutes</p>"},{"location":"kubedoom/#faq-frequently-asked-questionable-decisions","title":"FAQ: Frequently Asked Questionable Decisions","text":""},{"location":"kubedoom/#q-is-this-safe-for-production","title":"Q: Is this safe for production?","text":"<p>A: Define \"safe.\" Define \"production.\"</p>"},{"location":"kubedoom/#q-can-i-target-multiple-namespaces","title":"Q: Can I target multiple namespaces?","text":"<p>A: Not in the current configuration. For multiple namespaces, you'd need to deploy multiple instances. Then you can have co-op multiplayer. Then you can explain to your manager why your monitoring shows \"unusual pod churn.\"</p>"},{"location":"kubedoom/#q-what-happens-if-i-kill-a-database-pod","title":"Q: What happens if I kill a database pod?","text":"<p>A: The same thing that happens when you <code>kubectl delete</code> a database pod, but with more explosions and the Doom Guy's grunting. StatefulSets will handle it. Probably. Maybe check your backups first.</p>"},{"location":"kubedoom/#q-can-i-stream-this","title":"Q: Can I stream this?","text":"<p>A: Only if you're prepared to become either famous or unemployed. Possibly both.</p>"},{"location":"kubedoom/#q-my-boss-saw-me-playing-this-help","title":"Q: My boss saw me playing this. Help?","text":"<p>A: \"It's chaos engineering!\" \"We're testing pod resilience!\" \"This is active learning!\" Or just update your resume. Your call.</p>"},{"location":"kubedoom/#q-is-this-better-than-kubectl-delete-pod","title":"Q: Is this better than <code>kubectl delete pod</code>?","text":"<p>A: Objectively? No. Subjectively? Absolutely. Therapeutically? Without question.</p>"},{"location":"kubedoom/#the-philosophy-of-kube-doom","title":"The Philosophy of Kube DOOM","text":"<p>In the end, Kube DOOM teaches us valuable lessons:</p> <ol> <li>Pods are ephemeral - They live, they die, they respawn. Just like demons in Hell.</li> <li>Chaos builds resilience - If your app can't survive random termination, it can't survive production.</li> <li>DevOps should be fun - Who says cluster management has to be boring?</li> <li>Visual feedback matters - Sometimes you need to see the destruction to feel alive.</li> <li>Nostalgia is powerful - Combining 90s gaming with 2020s infrastructure is oddly satisfying.</li> </ol> <p>Ancient DevOps Proverb</p> <p>\"Rip and tear, until your cluster is stable.\" - Doom Guy, probably</p> <p>Remember: In the grim darkness of the Kubernetes cluster, there is only DOOM.</p> <p>Final Warning</p> <p>With great power comes great responsibility. With Kube DOOM comes great documentation for your incident post-mortem. Choose wisely.</p> <p>Now if you'll excuse me, I have some demons to kill in the <code>whoami</code> namespace. They know who they are.</p>"},{"location":"loadbalancer/","title":"Load Balancer Stack","text":"<p>The cluster exposes services to the home network through a combination of Traefik, KubeVIP, and Cilium. See Network Architecture for the full dataplane overview; this page focuses on the operational details for the load-balancing components.</p>"},{"location":"loadbalancer/#building-blocks","title":"Building Blocks","text":"<ul> <li>Traefik acts as the ingress controller and terminates TLS for HTTP(S) workloads.</li> <li>KubeVIP provides virtual IP addresses for both the Kubernetes API and <code>LoadBalancer</code> services.</li> <li>Cilium supplies the pod network, kube-proxy-free service routing, and transparent encryption but does not announce external VIPs in this setup.</li> </ul>"},{"location":"loadbalancer/#control-plane-virtual-ip","title":"Control-Plane Virtual IP","text":"<p>The DaemonSet in <code>apps/kubevip-ha/</code> keeps the Kubernetes API reachable under <code>https://192.168.1.230:6443</code>. Nodes labelled as control-plane members compete for leadership; the winner advertises the VIP on interface <code>eno1</code> using ARP.</p> <p>To rotate the static manifest:</p> <ol> <li>Pull the desired <code>ghcr.io/kube-vip/kube-vip</code> image.</li> <li>Generate a new DaemonSet manifest with the <code>manifest daemonset</code> helper.</li> <li>Copy the output into <code>apps/kubevip-ha/k8s.kubevip.yaml</code>, trimming <code>creationTimestamp</code> and <code>status</code> fields.</li> </ol> <p>Example using containerd:</p> <pre><code>export KVVERSION=v0.9.2\n/usr/local/bin/ctr image pull ghcr.io/kube-vip/kube-vip:${KVVERSION}\n/usr/local/bin/ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:${KVVERSION} vip /kube-vip \\\n  manifest daemonset \\\n  --interface eno1 \\\n  --address 192.168.1.230 \\\n  --inCluster \\\n  --taint \\\n  --controlplane \\\n  --services \\\n  --arp \\\n  --leaderElection \\\n  --enableNodeLabeling\n</code></pre> <p>GitHub releases: https://github.com/kube-vip/kube-vip/releases</p>"},{"location":"loadbalancer/#service-load-balancers","title":"Service Load Balancers","text":"<p>The deployment in <code>apps/kubevip-cloud-controller/</code> installs the kube-vip cloud controller manager. It watches every Service of type <code>LoadBalancer</code> and assigns a VIP from the range defined in the generated <code>kubevip</code> ConfigMap (<code>cidr-global=192.168.1.231-192.168.1.239</code> by default). Because Cilium L2 announcements are disabled, the kube-vip DaemonSet takes responsibility for advertising the VIP on whichever node currently hosts the matching backend pod \u2014 KubeVIP allows the address to \"follow\" the pod. Cilium currently doesn't have this feature.</p> <p>When you create a new external-facing service (for example Traefik or kube-prometheus-stack's Alertmanager):</p> <ol> <li>Set the Service <code>type: LoadBalancer</code>.</li> <li>Optional: pin a specific address by setting <code>spec.loadBalancerIP</code> to an IP inside the configured range.</li> <li>Watch the Service until the <code>EXTERNAL-IP</code> field displays a value from the pool.</li> <li>Configure DNS or clients to reach the assigned address.</li> </ol> <p>Although KubeVIP owns the VIP, the per-pod data plane is still provided by Cilium in hybrid DSR mode, so backend pods see the original client IP and respond directly, improving throughput.</p>"},{"location":"loadbalancer/#updating-the-cloud-controller","title":"Updating the Cloud Controller","text":"<p>The deployment references <code>ghcr.io/kube-vip/kube-vip-cloud-provider:v0.0.12</code>. To upgrade:</p> <ol> <li>Update the image tag in <code>apps/kubevip-cloud-controller/k8s.kube-vip-cloud-controller.yaml</code>.</li> <li>Apply the same change to any overlays in <code>apps/overlays/kubevip-cloud-controller/</code>.</li> <li>Commit and push; Argo CD will roll out the new deployment.</li> </ol>"},{"location":"loadbalancer/#troubleshooting-checklist","title":"Troubleshooting Checklist","text":"<ul> <li><code>kubectl -n kube-system get pods -l app.kubernetes.io/name=kube-vip-ds</code> \u2014 verify the control-plane DaemonSet is running on each leader-capable node.</li> <li><code>kubectl -n kube-system logs deploy/kube-vip-cloud-provider</code> \u2014 confirm Services receive VIP assignments.</li> <li><code>kubectl -n kube-system get configmap kubevip -o yaml</code> \u2014 verify the configured CIDR range matches your intended pool.</li> <li><code>ip addr show dev eno1 | rg 192.168.1.23</code> on a node \u2014 check which node currently owns the VIP.</li> <li><code>cilium service list</code> \u2014 confirm the service map contains the expected frontend (VIP) and backend pods.</li> </ul> <p>If ARP announcements stop, restart the kube-vip DaemonSet pod on the current leader and verify that no conflicting DHCP lease is advertising the same IP.</p>"},{"location":"mkdocs/","title":"mkdocs","text":"<p>This repository builds it's documentation with mkdocs.</p>"},{"location":"mkdocs/#installation","title":"Installation","text":"<pre><code>python3 -m venv --copies mkdocs\nsource mkdocs/bin/activate\n# Update all python packages within the python environment\npython3 -m pip list --outdated --format=json | jq -r '.[] | \"\\(.name)==\\(.latest_version)\"' | xargs --no-run-if-empty -n1 python3 -m pip install -U\n# Install mkdocs requirements\npip install -r requirements.txt\n</code></pre>"},{"location":"mkdocs/#building","title":"Building","text":"<pre><code>mkdocs build --strict --verbose\nmkdocs serve -a 127.0.0.1:8000\n</code></pre>"},{"location":"monitoring/","title":"Monitoring","text":""},{"location":"monitoring/#metrics","title":"Metrics","text":"<p>The kube-prometheus-stack helm chart pre-configures the following components:</p> <ul> <li>Prometheus (Operator)</li> <li>Grafana</li> <li>Node Exporter</li> <li>Alertmanager</li> <li>kube-state-metrics</li> </ul> <p></p> <p>Additionally the Alertmanager sends alerts via webhook to the ntfy-alertmanager which forwards it to a selfhosted ntfy.sh instance.</p> <p></p> <p>The encrypted config-file from the ntfy-alertmanager is basically this.</p> <p>More information can be found in the ntfy-alertmanager config documentation.</p> <pre><code>base-url https://ntfy-alertmanager.k8s.example.com\nhttp-address :8080\nlog-level info\nlog-format text\nalert-mode single\nuser &lt;SuperSecureUser&gt;\npassword &lt;SuperSecurePassword&gt;\nlabels {\n    order \"severity,instance\"\n    severity \"critical\" {\n        priority 5\n        tags \"rotating_light\"\n        icon \"\"\n    }\n    severity \"info\" {\n        priority 1\n    }\n}\nresolved {\n    tags \"resolved,partying_face\"\n    priority 1\n}\nntfy {\n    topic https://ntfy.example.com/kubernetes-at-home\n    access-token &lt;SuperSecureAccessToken&gt;\n}\nalertmanager {\n    silence-duration 24h\n    url http://kube-prometheus-stack-alertmanager.monitoring.svc.cluster.local:9093\n}\ncache {\n    type disabled\n    duration 24h\n    cleanup-interval 1h\n    redis-url redis://redis-master.databases.svc.cluster.local:6379/3\n}\n</code></pre>"},{"location":"monitoring/#logging","title":"Logging","text":"<p>Loki and promtail are used as pod log collector. The grafan helm repository provides multiple loki charts. <code>loki-stack</code> is outdated and shouldn't be used. The other interesting charts are grafana/loki (SSD Mode) and grafan/loki-distributed (microservice mode). More information on the modes (SSD vs. microservices): https://grafana.com/docs/loki/latest/get-started/deployment-modes/. This repository uses the SSD mode.</p> <p></p>"},{"location":"monitoring/#kubernetes-events","title":"Kubernetes Events","text":""},{"location":"network/","title":"Network Architecture","text":"<p>This cluster relies on a tightly integrated networking stack that combines Cilium for pod connectivity and security, KubeVIP for highly-available virtual IP addresses, and Traefik as the ingress controller fronted by Kubernetes <code>LoadBalancer</code> services. The following sections document how traffic moves through the environment and which manifests are responsible for each layer.</p>"},{"location":"network/#simplified-topology-diagram","title":"Simplified Topology Diagram","text":"<pre><code>                         Home LAN (192.168.1.0/24)\n                                      |\n                          +-------------------------+\n                          | Router / Default GW     |\n                          +-------------------------+\n                                      |\n                          +-------------------------+\n                          | VIP 192.168.1.230       |\n                          | kube-vip DaemonSet      |\n                          | (Kubernetes API)        |\n                          +-------------------------+\n                                      |\n          +---------------------------+---------------------------+\n          |                                                           |\n +----------------------+                                 +----------------------+\n | Control-plane node 1 |                                 | Control-plane node 2 |\n | eno1 / vmbr0         |                                 | eno1 / vmbr0         |\n +----------------------+                                 +----------------------+\n          |                                                           |\n          +---------------------------+---------------------------+\n                                      |\n                          +-------------------------+\n                          | kube-vip Cloud Ctrl     |\n                          | VIP pool 192.168.1.231-239 |\n                          +-------------------------+\n                                      |\n               +----------------------+----------------------+\n               |                                             |\n   +-----------------------+                     +-----------------------+\n   | Service: Traefik      |                     | Service: Traefik-vip239 |\n   | VIP 192.168.1.231     |                     | VIP 192.168.1.239       |\n   +-----------------------+                     +-----------------------+\n               |                                             |\n               +----------------------+----------------------+\n                                      |\n                          +-------------------------+\n                          | Cilium Data Plane       |\n                          | Pod CIDR 10.42.0.0/16   |\n                          | WireGuard encrypted     |\n                          +-------------------------+\n                                      |\n                          +-------------------------+\n                          | Workload Pods &amp; Hubble  |\n                          +-------------------------+\n</code></pre>"},{"location":"network/#underlay-topology","title":"Underlay Topology","text":"<ul> <li>All control-plane and worker nodes connect to the home LAN on <code>192.168.1.0/24</code>.</li> <li>A dedicated virtual IP <code>192.168.1.230</code> exposes the Kubernetes API and floats across control-plane nodes.</li> <li>Load-balancer service IPs are assigned from <code>192.168.1.231-192.168.1.239</code>, ensuring they never collide with DHCP-managed addresses on the LAN.</li> </ul> <p>Keeping these addresses outside of the DHCP scope prevents gratuitous ARP conflicts when KubeVIP announces ownership of a VIP.</p>"},{"location":"network/#pod-networking-with-cilium","title":"Pod Networking with Cilium","text":"<p>Cilium is deployed via Argo CD (<code>apps/argo-cd-apps-helm/00-cilium.yaml</code>) and replaces both kube-proxy and the default flannel CNI that ships with K3s.</p> <ul> <li>eBPF data plane: <code>kubeProxyReplacement: true</code> enables kube-proxy-free service routing, reducing hops and relying on eBPF programs instead of iptables.</li> <li>Pod CIDR: The IPAM operator manages the <code>10.42.0.0/16</code> pod network (<code>clusterPoolIPv4PodCIDRList</code>).</li> <li>Encryption: WireGuard is enabled (<code>encryption.type: wireguard</code>), so pod-to-pod traffic inside the cluster is transparently encrypted at Layer 3.</li> <li>Devices: The agents are bound to the physical interfaces <code>eno1 vmbr0</code> so all relevant NICs participate in datapath processing.</li> <li>Tunneling: The GENEVE tunnel protocol allows encrypted overlay traffic across different subnets while preserving eBPF optimisation.</li> <li>Observability: Hubble Relay and Hubble UI are enabled for flow inspection.</li> </ul> <p>Note Cilium's experimental Layer 2 announcement feature is disabled (<code>l2announcements.enabled: false</code>) so that VIP ownership can be handled entirely by KubeVIP. This guarantees the external IP can move to the node hosting the active backend pod, something Cilium cannot yet do.</p>"},{"location":"network/#network-policy","title":"Network Policy","text":"<p>Every ArgoCD App comes with its own sets of CiliumNetworkPolicies, allowing only the required incoming and outgoing pod connections.</p>"},{"location":"network/#kubevip-roles","title":"KubeVIP Roles","text":"<p>KubeVIP is installed in two distinct modes, both orchestrated by Argo CD:</p> <ol> <li> <p>Control-plane virtual IP (<code>apps/kubevip-ha</code>)    - Runs as a DaemonSet on nodes labelled <code>node-role.kubernetes.io/master</code> or <code>node-role.kubernetes.io/control-plane</code>.    - Advertises <code>192.168.1.230</code> on interface <code>eno1</code>, presenting a single stable endpoint for <code>https://&lt;vip&gt;:6443</code>.    - Leader election is configured with rapid timeouts (<code>vip_leaseduration: 5</code>, <code>vip_renewdeadline: 3</code>, <code>vip_retryperiod: 1</code>) to minimise failover time.</p> </li> <li> <p>Cloud controller (<code>apps/kubevip-cloud-controller</code>)    - Watches Kubernetes <code>Service</code> objects of type <code>LoadBalancer</code> and annotates them with a VIP from the configured pool.    - Runs as a singleton deployment using image <code>ghcr.io/kube-vip/kube-vip-cloud-provider:&lt;tag&gt;</code>.    - Collaborates with Cilium: KubeVIP assigns the IP, updates Service status, and advertises the VIP, while Cilium continues to handle pod networking and service routing inside the cluster.</p> </li> </ol>"},{"location":"network/#updating-kubevip","title":"Updating KubeVIP","text":"<p>Follow the instructions in <code>docs/loadbalancer.md</code> to regenerate manifests when upgrading to newer KubeVIP releases. Always update the DaemonSet image tag and the cloud-controller deployment in lockstep.</p>"},{"location":"network/#load-balancing-workflow","title":"Load Balancing Workflow","text":"<ol> <li>You declare a service (for example Traefik) as <code>type: LoadBalancer</code>.</li> <li>The kube-vip cloud controller allocates a free IP from the <code>cidr-global</code> range defined in the <code>kubevip</code> ConfigMap and records it in the Service status.</li> <li>The kube-vip DaemonSet elects the node that should host the VIP\u2014preferentially the node running the backend pods\u2014and advertises the address on the local network using ARP.</li> <li>Client traffic from the LAN reaches that node, where KubeVIP performs the load-balancing decision and hands the connection to the target pod.</li> </ol> <p>This layered approach yields fast failovers, keeps the control plane highly available, and avoids the need for external load balancer appliances.</p>"},{"location":"network/#traefik-virtual-ip-mapping","title":"Traefik Virtual IP Mapping","text":"<p>The Traefik release in <code>apps/traefik/kustomization.yaml</code> exposes two independent LoadBalancer frontends backed by kube-vip:</p> <ul> <li><code>traefik</code> (LoadBalancer IP <code>192.168.1.231</code>) publishes the <code>web</code> and <code>websecure</code> entrypoints for general ingress traffic.</li> <li><code>traefik-vip239</code> (LoadBalancer IP <code>192.168.1.239</code>) publishes the dedicated <code>web239</code> and <code>websecure239</code> entrypoints for workloads that must live on the second VIP.</li> </ul> <p>Ingress manifests remain otherwise unchanged; assigning them to the secondary IP only requires the annotation <code>traefik.ingress.kubernetes.io/router.entrypoints: web239,websecure239</code>. Traefik listens on separate container ports for these entrypoints, so kube-vip can pin each Service to the correct address without additional node-level networking changes.</p>"},{"location":"network/#extending-the-network-stack","title":"Extending the Network Stack","text":"<ul> <li>New VIP ranges: Update the <code>cidr-global</code> literal in <code>apps/kubevip-cloud-controller/kustomization.yaml</code> and document the range in your network inventory.</li> <li>Additional interfaces: Modify the <code>devices</code> setting in the Helm values if you add NICs to your nodes.</li> <li>Fine-grained policy: Start from the sample manifests under <code>apps/cilium-network-policies/</code> and deploy them via Argo CD to enforce zero-trust defaults.</li> <li>Testing: Use Hubble UI or <code>cilium status</code>/<code>cilium connectivity test</code> on a node to validate connectivity after changes.</li> </ul> <p>Service IP address management is configured through the <code>kubevip</code> ConfigMap generated in <code>apps/kubevip-cloud-controller/kustomization.yaml</code>. When adding an IP range, update the <code>cidr-global</code> literal and keep it aligned with the DHCP exclusions on your router or firewall.</p> <p>For component-specific procedures (such as regenerating KubeVIP manifests), refer to LoadBalancer.</p>"},{"location":"network/#troubleshooting","title":"Troubleshooting","text":"<p><code>hubble observe</code> provides real-time insight in the network flow and allows filtering by e.g. dropped frames.</p> <pre><code>hubble observe --last 20 --namespace kube-system --verdict DROPPED -f\n</code></pre>"},{"location":"pre-commit-hooks/","title":"pre-commit","text":"<p>This repository uses pre-commit to manage pre-commit hooks.</p>"},{"location":"pre-commit-hooks/#installation","title":"Installation","text":"<p>Installing the binary is covered on the homepage of pre-commit.</p> <p>To activate it for this repository:</p> <pre><code>pre-commit install\n</code></pre> <p>pre-commits are managed in the <code>.pre-commit-config.yaml</code> file.</p> <p>Run a test over all files:</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"reflector/","title":"Reflector","text":"<p>Reflector  syncs Secrets and ConfigMaps between namespaces. I use it to sync the following objects between namespaces:</p> <ul> <li>Wildcard certificate</li> <li>Backup script</li> </ul>"},{"location":"secretmanagement/","title":"Secret Management","text":"<p>In this repository I use two ways to encrypt secrets, both utilizing sops and age.</p> <ul> <li>Kubernetes Secrets / Manifests are encrypted via KSOPS (described in this document)</li> <li>Kustomize managed Helm values are encrypted via sops and decrypted by an ArgoCD ConfigManagementPlugin</li> </ul> <p>age is the recommended encryption tool for sops as it is more secure and easier to use than gpg.</p>"},{"location":"secretmanagement/#requirements","title":"Requirements","text":"<ul> <li>ksops</li> <li>age</li> </ul>"},{"location":"secretmanagement/#preparation","title":"Preparation","text":"<p>For both variants we need two age keypairs. One for local use and one for ArgoCD.</p> <pre><code># the folder for the age keypairs, consumed by sops\nmkdir -p \"$HOME/.config/sops/age\"\n# local age keypair\nage-keygen -o \"$HOME/.config/sops/age/keys.txt\"\n# argocd age keypair\nage-keygen -o \"$HOME/.config/sops/age/argo-cd.txt\"\n</code></pre> <p>Example output:</p> <pre><code>cat \"$HOME/.config/sops/age/keys.txt\"\n# created: 2024-05-28T07:23:28+02:00\n# public key: age***\nAGE-SECRET-KEY-19***\n</code></pre> <p>ArgoCD needs the private key of the local keypair to decrypt the secrets. So we create a kubernetes secret with the private key that ArgoCD gets mounted.</p> <pre><code>cat \"$HOME/.config/sops/age/argo-cd.txt\" | kubectl create secret generic sops-age --namespace=argocd \\\n--from-file=keys.txt=/dev/stdin\n</code></pre> <p>Adjusted helm values to mount the sops-age secret into the argocd-server pod:</p> values.yaml<pre><code>repoServer:\n  volumes:\n    - name: sops-age\n      secret:\n        secretName: sops-age\n  volumeMounts:\n    - mountPath: /.config/sops/age\n      name: sops-age\n      readOnly: true\n  env:\n    - name: SOPS_AGE_KEY_FILE\n      value: /.config/sops/age/keys.txt\n</code></pre>"},{"location":"secretmanagement/#repository-configuration","title":"Repository configuration","text":"<p>Info</p> <p>The secrets need to be encrypted with both public keys. The ArgoCD key is used to decrypt the secrets in the ArgoCD cluster and the local key is used to de- and encrypt the secrets locally.</p> <p>Create a <code>.sops.yaml</code> file in the repository root. Example:</p> .sops.yaml<pre><code>creation_rules:\n  - path_regex: .*.enc.yaml\n    encrypted_regex: \"^(data|stringData|email|dnsNames|.*(H|h)osts?|hostname|username|password|url|issuer|clientSecret|argocdServerAdminPassword|oidc.config|commonName|literals)$\"\n    age: age1d2g7tgqpfvxulsusn3m608h60h2hne7yqwv5nh5nd24z6h0hgq0skjkhw8,age1q522xtgjrmvr43w7um5rh02ta3yfns635680hz4m7uhw0nfqj5zqgxnz27\n  - path_regex: secrets/argo-cd.age\n    age: age1d2g7tgqpfvxulsusn3m608h60h2hne7yqwv5nh5nd24z6h0hgq0skjkhw8\n</code></pre> <p>The .sops file for this repository differs from this example.</p>"},{"location":"secretmanagement/#kubernetes-secrets-manifests-via-ksops","title":"Kubernetes Secrets / Manifests via KSOPS","text":"<p>To use sops with ArgoCD, you need to mount ksops and the sops-age key into the argocd-server pod. The following helm values start the ksops container as an initContainer, copies the ksops and kustomize binaries into the custom-tools volume and mounts the binaries from the custom-tools volume into the repo server container. The sops-age key is also mounted into the repo server container.</p> values.yaml<pre><code>configs:\n  cm:\n    kustomize.buildOptions: \"--enable-helm --enable-alpha-plugins --enable-exec\"\nrepoServer:\n  # Use init containers to configure custom tooling\n  # https://argoproj.github.io/argo-cd/operator-manual/custom_tools/\n  volumes:\n    - name: custom-tools\n      emptyDir: {}\n  initContainers:\n    - name: install-ksops\n      image: viaductoss/ksops:v4.3.1\n      command: [\"/bin/sh\", \"-c\"]\n      args:\n        - echo \"Installing KSOPS...\";\n          mv ksops /custom-tools/;\n          mv kustomize /custom-tools/;\n          echo \"Done.\";\n      volumeMounts:\n        - mountPath: /custom-tools\n          name: custom-tools\n  volumeMounts:\n    # ksops packages it's own kustomize binary with ksops integration, overrides the argocd kustomize binary\n    - mountPath: /usr/local/bin/kustomize\n      name: custom-tools\n      subPath: kustomize\n    - mountPath: /usr/local/bin/ksops\n      name: custom-tools\n      subPath: ksops\n    - mountPath: /.config/sops/age\n      name: sops-age\n  env:\n    - name: XDG_CONFIG_HOME\n      value: /.config\n</code></pre>"},{"location":"secretmanagement/#adjusting-kustomize-configuration","title":"Adjusting kustomize configuration","text":"<p>Info</p> <p>To tell kustomize to use ksops for decryption, we need to add a generators configuration to the <code>kustomization.yaml</code> file.</p> kustomization.yaml<pre><code>generators:\n  - kustomize-secret-generator.yaml\n</code></pre> kustomize-secret-generator.yaml<pre><code>---\napiVersion: viaduct.ai/v1\nkind: ksops\nmetadata:\n  name: secret-generator\n  annotations:\n    config.kubernetes.io/function: |\n      exec:\n        path: ksops\nfiles:\n  - backup-secrets.enc.yaml\n</code></pre> <p>The backup-secrets.enc.yaml is just a normal kubernetes manifest but with sops encrypted values: Example emby backup-secrets.enc.yaml. Do not add thouse manifests to the resource list of the kustomization.yaml because this would result in a duplicate resource error.</p>"},{"location":"secretmanagement/#kustomize-managed-helm-values","title":"Kustomize managed Helm values","text":"<p>This repository makes heavy use of kustomize rendering helm charts. Kustomize can manage helm values either directly in the kustomization.yaml or in a separate file. The helm values can contain sensitive values and it's not possible to encrypt values in the kustomization.yaml file directly so we need to use a separate helm values file.</p> <p>Info</p> <p>The workflow basically is to create an encrypted values.enc.yaml, tell kustomize to get the helm values from values.yaml and use an ArgoCD ConfigManagementPlugin to decrypt the values.enc.yaml to values.yaml. The ConfigManagementPlugin gets executed when ArgoCD finds a values.enc.yaml before kustomize renders the kubernetes manifests.</p>"},{"location":"secretmanagement/#configuration-of-the-configmanagementplugin","title":"Configuration of the ConfigManagementPlugin","text":"<p>If ArgoCD finds a values.enc.yaml in an application directory, argo-cd runs the CMP cmp-sops-decrypt, which decryptes the file to values.yaml, and then runs kustomize.</p> <p>The ConfigManagementPlugin is configured as a configmap within a separate argo-cd app deployment. Additionaly it needs helm and sops binaries for the argo-cd repo-server which get configured via a sidecar container. So the deployment of it needs to be extended.</p> <p>ConfigManagementPlugin:</p> argocd-cmp-sops-plugin.yaml<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-cmp-sops-plugin\n  namespace: argocd\ndata:\n  plugin.yaml: |\n    ---\n    apiVersion: argoproj.io/v1alpha1\n    kind: ConfigManagementPlugin\n    metadata:\n      name: cmp-sops-decrypt\n    spec:\n      version: v1.0\n      generate:\n        command: [sh, -c]\n        args:\n          - sops --decrypt --input-type yaml --output-type yaml values.enc.yaml &gt; values.yaml;\n            kustomize build --enable-helm --enable-alpha-plugins --enable-exec .\n      discover:\n        fileName: \"values.enc.yaml\"\n</code></pre> <p>The adjusted helm values for the argo-cd repo-server. Some of the adjustments are dependent on changes from the previous section (like ksops and kustomize usage).</p> values.yaml<pre><code>repoServer:\n  # Addingcustom tools volume and ConfigManagementPlugin to the repo-server pod deployment\n  volumes:\n    - name: custom-tools\n      emptyDir: {}\n    - name: cmp-tmp\n      emptyDir: {}\n    - name: cmp-sops-plugin\n      configMap:\n        name: argocd-cmp-sops-plugin\n  # Installation of binaries\n  initContainers:\n    - name: install-sops\n      image: ghcr.io/getsops/sops:v3.8.1-alpine\n      command:\n        - /bin/sh\n        - -c\n      args:\n        - echo \"Installing SOPS...\";\n          cp /usr/local/bin/sops /custom-tools/;\n          echo \"Done.\";\n      volumeMounts:\n        - mountPath: /custom-tools\n          name: custom-tools\n    - name: install-helm\n      image: alpine/helm:3.15.1\n      command:\n        - /bin/sh\n        - -c\n      args:\n        - echo \"Installing helm...\"; cp /usr/bin/helm /custom-tools/; echo \"Done.\";\n      volumeMounts:\n        - mountPath: /custom-tools\n          name: custom-tools\n  # Adding Container responsible for the configured ConfigManagementPlugin\n  extraContainers:\n    - name: cmp-sops-plugin\n      command:\n        - \"/var/run/argocd/argocd-cmp-server\"\n      image: alpine:3.20.0\n      imagePullPolicy: IfNotPresent\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n      volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /home/argocd/cmp-server/config/plugin.yaml\n          subPath: plugin.yaml\n          name: cmp-sops-plugin\n        - mountPath: /tmp\n          name: cmp-tmp\n        - mountPath: /usr/local/bin/kustomize\n          name: custom-tools\n          subPath: kustomize\n        - mountPath: /usr/local/bin/ksops\n          name: custom-tools\n          subPath: ksops\n        - mountPath: /usr/local/bin/sops\n          name: custom-tools\n          subPath: sops\n        - mountPath: /usr/local/bin/helm\n          name: custom-tools\n          subPath: helm\n        - mountPath: /.config/sops/age\n          name: sops-age\n          readOnly: true\n  volumeMounts:\n    - mountPath: /usr/local/bin/kustomize\n      name: custom-tools\n      subPath: kustomize\n    - mountPath: /usr/local/bin/ksops\n      name: custom-tools\n      subPath: ksops\n    - mountPath: /.config/sops/age\n      name: sops-age\n      readOnly: true\n</code></pre> <p>This basically builds the plugin container with all required tools on-demand.</p> <p>Of course, the configuration could be way shorter if a container, that already includes the following binaries, would be used as extraContainer \ud83e\udd37</p> <ul> <li>kustomize (from ksops)</li> <li>ksops</li> <li>sops</li> <li>helm</li> </ul> <p>More information about my journey to en- and decrypt values.yaml can be found in the following ksops issue on github: Support kustomize helmCharts valuesFile.</p>"},{"location":"secretmanagement/#en-and-decrypting-helm-values-and-manifests","title":"En- and decrypting helm values and manifests","text":"<p>To encrypt a file inplace, use the following command:</p> <pre><code>sops -e -i secret.enc.yaml\n</code></pre> <p>To decrypt a file inplace, use the following command:</p> <pre><code>sops -d -i secret.enc.yaml\n</code></pre> <p>Tip</p> <p>If you're working with VSCode I can recommend the extension @signageos/vscode-sops which automatically decrypts and encrypts secrets on save.</p> <p>It can also automatically encrypt files which are not yet encrypted. To enable this feature, add the following to your <code>settings.json</code>:</p> <pre><code>{\n  \"sops.creationEnabled\": true\n}\n</code></pre> <p>This will automatically encrypt files which match the <code>creation_rules</code> in the <code>.sops.yaml</code> file.</p>"},{"location":"secrets/","title":"Kubernetes Secrets erstellen","text":""},{"location":"secrets/#via-kubernetes-manifest","title":"Via Kubernetes Manifest","text":"<p>Base64 encoded:</p> <pre><code>echo -n '1f2d1e2e67df' | base64\nMWYyZDFlMmU2N2Rm\n</code></pre> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\ndata:\n  password: MWYyZDFlMmU2N2Rm\n</code></pre> <p>Plain Text</p> <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: mysecret\ntype: Opaque\nstringData:\n  password: 1f2d1e2e67df\n</code></pre>"},{"location":"secrets/#links","title":"Links","text":"<ul> <li>https://www.mirantis.com/cloud-native-concepts/getting-started-with-kubernetes/what-are-kubernetes-secrets/</li> </ul>"},{"location":"storage/","title":"Distributed Storage","text":""},{"location":"storage/#longhorn","title":"Longhorn","text":"<p>Longhorn is used as primary local distributed storage.</p>"},{"location":"storage/#troubleshooting","title":"Troubleshooting","text":"<p>If a pod can't start and in the events is something like \"Volume is already exclusively attached to one node and can't be attached to anothe\" you need to wait at least 2 hours. Haven't found a solution yet.</p>"},{"location":"testenv/","title":"Test environment","text":"<p>The test environment is based upon virtual machines and created with Vagrant and libvirt. You need a quiet powerful machine to run the full test environment. It can create up to three systems with the following configuration:</p>"},{"location":"testenv/#virtual-machine-specifications","title":"Virtual Machine Specifications","text":"<ul> <li>k3svm1: The server node of the k3s cluster<ul> <li>4096MB RAM</li> <li>4 CPU threads</li> <li>Role: k3s server (control plane)</li> </ul> </li> <li>k3svm2: The first worker node of the k3s cluster<ul> <li>2048MB RAM</li> <li>2 CPU threads</li> <li>Role: k3s server (control plane)</li> </ul> </li> <li>k3svm3: The second worker node of the k3s cluster<ul> <li>2048MB RAM</li> <li>2 CPU threads</li> <li>Role: k3s agent (worker)</li> </ul> </li> </ul> <p>Total resources required: 8192MB RAM and 8 CPU threads when running all three VMs.</p>"},{"location":"testenv/#architecture-decision","title":"Architecture Decision","text":"<p>I decided to base the test environment on virtual machines because I also test different network plugins. This is much easier to do with virtual machines than with containers. Another requirement is ansible, which I use to initially install k3s. This also needs to be tested. During deployment Vagrant uses the same ansible playbook to provision the system(s) as the one used for the production environment. Just the vars are a bit different and can be adjusted in the <code>ansible/inventory/vagrant/group_vars/all/main.yaml</code> file.</p>"},{"location":"testenv/#file-organization","title":"File Organization","text":"<p>In the subfolder ./shared/${HOSTNAME}/ are files for the configuration of the k3s cluster, like the kubeconfig.</p> <p>The folder ./shared gets also mounted to the virtual machines. This is the place where you can put files you want to share with the virtual machines. Though this requires memory_backing_dir = \"/dev/shm\" in /etc/libvirt/qemu.conf.</p> <p>libvirt Configuration Required</p> <p>The shared folder mounting requires the following configuration in /etc/libvirt/qemu.conf: <pre><code>memory_backing_dir = \"/dev/shm\"\n</code></pre> After changing this setting, restart libvirtd: <pre><code>sudo systemctl restart libvirtd\n</code></pre></p>"},{"location":"testenv/#prerequisites","title":"Prerequisites","text":"<p>Before setting up the test environment, ensure you have:</p> <ul> <li>Virtualization support: CPU with VT-x/AMD-V enabled in BIOS</li> <li>libvirt: KVM/QEMU virtualization platform installed</li> <li>Vagrant: Version 2.3.0 or higher</li> <li>Sufficient resources: At least 8GB free RAM and 8 CPU threads for full cluster</li> <li>Disk space: Minimum 20GB free for VM images and data</li> </ul>"},{"location":"testenv/#setting-up-vagrant","title":"Setting up vagrant","text":"<p>At first install vagrant for your system. This wont be covered here. Please refer to the vagrant documentation.</p> <p>Add the libvirt plugin:</p> <pre><code>vagrant plugin install vagrant-libvirt\n</code></pre> <p>Using Docker Container (Recommended)</p> <p>If you have, like me, dependency problems with the libvirt plugin, you can use the vagrant-libvirt docker container. I recommend you to use my extended vagrant-libvirt-ansible container with added ansible. Ansible is required for provisioning and by default not installed in the standard vagrant-libvirt container.</p> <p>To use the container: <pre><code>docker run -it --rm \\\n  -v /var/run/libvirt/:/var/run/libvirt/ \\\n  -v ~/.vagrant.d:/root/.vagrant.d \\\n  -v $(pwd):$(pwd) \\\n  -w $(pwd) \\\n  --network host \\\n  ghcr.io/madic-creates/vagrant-libvirt-ansible:latest \\\n  vagrant up\n</code></pre></p>"},{"location":"testenv/#starting-the-test-environment","title":"Starting the Test Environment","text":""},{"location":"testenv/#full-cluster-deployment","title":"Full Cluster Deployment","text":"<p>Start the complete test environment with all three nodes:</p> <pre><code>vagrant up\n</code></pre> <p>This will:</p> <ol> <li>Create three VMs (k3svm1, k3svm2, k3svm3)</li> <li>Run the Ansible playbook <code>ansible/playbooks/install.yaml</code></li> <li>Install k3s on the server node</li> <li>Join worker nodes to the cluster</li> <li>Copy kubeconfig to <code>shared/k3svm1/k3s.yaml</code></li> </ol> <p>Provisioning Time</p> <p>The initial provisioning can take 10-15 minutes depending on your system resources and internet connection. After the Vagrant provisioning completes, allow an additional 2-3 minutes for k3s to fully initialize.</p>"},{"location":"testenv/#single-node-deployment","title":"Single Node Deployment","text":"<p>Start only the server node (which is enough for many tests):</p> <pre><code>vagrant up k3svm1\n</code></pre> <p>This creates a single-node cluster that is sufficient for testing:</p> <ul> <li>Application deployments</li> <li>ArgoCD functionality</li> <li>Kustomize configurations</li> <li>Secret management with SOPS</li> <li>Basic networking</li> </ul>"},{"location":"testenv/#accessing-the-cluster","title":"Accessing the Cluster","text":"<p>After the installation is finished, it can still take some time till the k3s service is up and running.</p> <p>The ansible ansible/playbooks/install.yaml playbook, which vagrant automatically runs, will copy the kubeconfig from the server node to <code>shared/k3svm1/k3s.yaml</code>. You can use this file to access the test k3s cluster.</p> <p>Set the KUBECONFIG environment variable:</p> <pre><code>export KUBECONFIG=\"$PWD/shared/k3svm1/k3s.yaml\"\n</code></pre> <p>Verify cluster access:</p> <pre><code>kubectl get nodes\nkubectl get pods -A\n</code></pre>"},{"location":"testenv/#adding-additional-nodes","title":"Adding additional Nodes","text":"<p>Start the worker nodes separately if needed. Ansible will automatically join the worker nodes to the k3s cluster.</p> <pre><code>vagrant up k3svm2\nvagrant up k3svm3\n</code></pre> <p>When to Use Worker Nodes</p> <p>Worker nodes are useful for testing:</p> <ul> <li>Multi-node scheduling</li> <li>Pod anti-affinity rules</li> <li>Node selectors and taints</li> <li>Storage replication (Longhorn)</li> <li>High availability scenarios</li> </ul>"},{"location":"testenv/#ssh-access","title":"SSH Access","text":"<p>Once your virtual machine is up and running, you can log in to it:</p> <pre><code>vagrant ssh k3svm1\n</code></pre> <p>Useful commands inside the VM:</p> <pre><code># Check k3s service status\nsudo systemctl status k3s\n\n# View k3s logs on control-plane\nsudo journalctl -u k3s -f\n\n# View k3s logs on worker\nsudo journalctl -u k3s-agent -f\n\n# Check node resources\nkubectl top nodes\n</code></pre>"},{"location":"testenv/#destroying-the-environment","title":"Destroying the Environment","text":"<p>Destroy the test environment and reclaim resources:</p> <pre><code>vagrant destroy -f\n</code></pre> <p>To destroy a specific VM:</p> <pre><code>vagrant destroy k3svm2 -f\n</code></pre>"},{"location":"testenv/#testing-workflows","title":"Testing Workflows","text":""},{"location":"testenv/#deploying-applications","title":"Deploying Applications","text":"<p>Once the cluster is running, you can test application deployments:</p> <pre><code># Build kustomization locally\nkubectl kustomize apps/myapp --enable-helm\n\n# Apply directly (for testing without ArgoCD)\nkubectl kustomize apps/myapp --enable-helm | kubectl apply -f -\n\n# Check deployment status\nkubectl get pods -n myapp-namespace\nkubectl describe pod &lt;pod-name&gt; -n myapp-namespace\n</code></pre>"},{"location":"testenv/#testing-argocd","title":"Testing ArgoCD","text":"<p>Deploy ArgoCD to the test cluster:</p> <pre><code># Deploy ArgoCD\nkubectl kustomize apps/argo-cd --enable-helm | kubectl apply -f -\n\n# Wait for ArgoCD to be ready\nkubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd\n\n# Get admin password\nkubectl get secret argocd-initial-admin-secret -n argocd -o jsonpath=\"{.data.password}\" | base64 -d\n</code></pre> <p>Port-forward to access ArgoCD UI:</p> <pre><code>kubectl port-forward svc/argocd-server -n argocd 8080:443\n</code></pre>"},{"location":"testenv/#testing-network-policies","title":"Testing Network Policies","text":"<p>When testing Cilium network policies, use Hubble:</p> <pre><code># Install Cilium CLI in the VM\nvagrant ssh k3svm1\ncurl -L --remote-name-all https://github.com/cilium/cilium-cli/releases/latest/download/cilium-linux-amd64.tar.gz\nsudo tar xzvfC cilium-linux-amd64.tar.gz /usr/local/bin\nrm cilium-linux-amd64.tar.gz\n\n# Enable Hubble port-forward\ncilium hubble port-forward &amp;\n\n# Observe traffic from your namespace\nhubble observe --namespace myapp --last 100\n</code></pre>"},{"location":"testenv/#vagrant-snapshots","title":"Vagrant Snapshots","text":"<p>Create snapshots for quick rollback:</p> <pre><code># Take a snapshot\nvagrant snapshot save k3svm1 clean-state\n\n# List snapshots\nvagrant snapshot list\n\n# Restore snapshot\nvagrant snapshot restore k3svm1 clean-state\n\n# Delete snapshot\nvagrant snapshot delete k3svm1 clean-state\n</code></pre>"},{"location":"testenv/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testenv/#vagrant-issues","title":"Vagrant Issues","text":"<p>Sometimes vagrant has conflicts with OS packages resulting in messages like this:</p> <p>conflicting dependencies date (= 3.2.2) and date (= 3.3.4)</p> <p>Set the following environment variable to ignore gem versions or directly use my vagrant-libvirt-ansible container:</p> <pre><code>export VAGRANT_DISABLE_STRICT_DEPENDENCY_ENFORCEMENT=1\n</code></pre>"},{"location":"testenv/#vm-wont-start","title":"VM Won't Start","text":"<p>Check libvirt status:</p> <pre><code>sudo systemctl status libvirtd\nsudo virsh list --all\n</code></pre> <p>View libvirt logs:</p> <pre><code>sudo journalctl -u libvirtd -f\n</code></pre>"},{"location":"testenv/#k3s-service-not-starting","title":"k3s Service Not Starting","text":"<p>SSH into the VM and check k3s status:</p> <pre><code>vagrant ssh k3svm1\nsudo systemctl status k3s\nsudo journalctl -u k3s --no-pager -n 50\n</code></pre>"},{"location":"testenv/#kubeconfig-not-working","title":"Kubeconfig Not Working","text":"<p>Ensure the kubeconfig file exists:</p> <pre><code>ls -la shared/k3svm1/k3s.yaml\n</code></pre> <p>If missing, manually copy from VM:</p> <pre><code>vagrant ssh k3svm1 -c \"sudo cat /etc/rancher/k3s/k3s.yaml\" | sed \"s/127.0.0.1/$(vagrant ssh-config k3svm1 | grep HostName | awk '{print $2}')/g\" &gt; shared/k3svm1/k3s.yaml\n</code></pre>"},{"location":"testenv/#cleaning-up","title":"Cleaning Up","text":"<p>If VMs are in an inconsistent state:</p> <pre><code># Stop all VMs\nvagrant halt\n\n# Destroy all VMs\nvagrant destroy -f\n\n# Clean libvirt domains\nsudo virsh list --all\nsudo virsh undefine k3svm1\nsudo virsh undefine k3svm2\nsudo virsh undefine k3svm3\n\n# Remove storage\nsudo virsh vol-list default\nsudo virsh vol-delete k3svm1.img default\n</code></pre>"},{"location":"testenv/#performance-optimization","title":"Performance Optimization","text":""},{"location":"testenv/#reducing-resource-usage","title":"Reducing Resource Usage","text":"<p>For limited hardware, modify the Vagrantfile to reduce resources:</p> <pre><code># Example modifications\nconfig.vm.define \"k3svm1\" do |node|\n  node.vm.provider :libvirt do |libvirt|\n    libvirt.memory = 2048  # Reduced from 4096\n    libvirt.cpus = 2       # Reduced from 4\n  end\nend\n</code></pre>"},{"location":"updates/","title":"Update management","text":"<p>The cluster uses two components to manage updates:</p> <ul> <li>Renovate</li> <li>System Upgrade Controller</li> </ul>"},{"location":"updates/#renovate","title":"Renovate","text":"<p>Renovate is configured as a scheduled github workflow. Additionally, the action can be run manually with optional increased debug log or in dry-run mode. As RENOVATE_TOKEN a github personal access token is used.</p> <p>More information about the required token permissions can be found in the official docs: https://docs.renovatebot.com/modules/platform/github/. I keep it here because I had a hard time finding thoses information \ud83d\ude48</p>"},{"location":"updates/#rules","title":"Rules","text":"<p>Multiple configurations in the renovate.json support renovate in finding updates.</p> <p>Kubernetes Manifests</p> <p>Renovate identifies Kubernetes manifests depending on the file name. The current regex is <code>apps\\/.+\\/k8s\\\\..*\\\\.yaml$</code>. Basically beneath the apps folder it matches every yaml file that begins with k8s.</p> <p>Danger</p> <p>Renovate must not change Kubernetes manifests which contain sops encrypted entries because that would break the file signature. Encrypted Kubernetes manifests aren't allowed to begin with k8s!</p> <p>Auto Updates</p> <p>Renovate will auto-update every entry after 5 days of release. If not explicitly excluded, every minor and patch release gets automatically updated / merged. Major releases need to be approved via Pull Request.</p> <p>Longhorn is an exception. It will never automerge and will only create Pull Requests for releases that are available for 14 days, even for patch releases. Because Longhorn is the storage provider for the cluster, I want it to be a tested release.</p> <p>Image Tags</p> <p>Beside updating image tags in kubernetes manifests, renovate updates image tags in kustomization.yaml files when it finds the keyword <code>image</code> or the following structure:</p> <pre><code>repository: emby/embyserver\ntag: 4.9.0.30\n</code></pre> <p><code>Repository</code> is required for renovate to know, in which repository to search for new versions to update the tag.</p> <p>pre-commit hooks</p> <p>Renovate is able to update pre-commit hooks. But it's still in beta and can lead to problems. Because of this automerge for pre-commit is disabled and every update requires approval through a pull request.</p>"},{"location":"updates/#system-upgrade-controller","title":"System Upgrade Controller","text":"<p>The System Upgrade Controller is a Kubernetes controller that manages the upgrade of a Kubernetes cluster. There are only plans configured to automatically upgrade the cluster to the latest k3s version.</p> <p>The update plans are a separate ArgoCD app because they require CRDs from the system-upgrade-controller. Without those CRDs ArgoCD would not be able to apply the plans and the deployment would fail if they are included in the system-upgrade-controller app.</p>"},{"location":"operations/cilium-migration/","title":"Networking","text":""},{"location":"operations/cilium-migration/#replacing-flannel-with-cilium-live","title":"Replacing flannel with cilium - live","text":"<p>In the beginning the cluster used flannel as cni. I wanted to use cilium so I tested a live migration from flannel to cilium. In short: Only do this if you realy know what you are doing. This is a complex task and requires linux and kubernetes networking knowledge. The following guide is just a starting point. There were many small problems afterwards.</p>"},{"location":"operations/cilium-migration/#creating-test-environment","title":"Creating test environment","text":"<p>In repository root</p> <pre><code>vagrant up\n# Wait vor all hosts becoming ready\ncd apps/overlays/\nexport KUBECONFIG=\"$(realpath ../../shared/k3svm1/k3s.yaml)\"\nkustomize build --enable-alpha-plugins --enable-exec ./kubevip-cloud-controller | kubectl apply -f -\nkustomize build --enable-alpha-plugins --enable-exec ./kubevip-ha | kubectl apply -f -\nkustomize build --enable-alpha-plugins --enable-exec ./traefik | kubectl apply -f -\nkustomize build --enable-alpha-plugins --enable-exec ./debug-container | kubectl apply -f -\nkustomize build --enable-alpha-plugins --enable-exec ./whoami | kubectl apply -f -\n</code></pre>"},{"location":"operations/cilium-migration/#migration-phase","title":"Migration phase","text":"<p>On all server nodes:</p> <pre><code>systemctl stop k3s.service\n</code></pre> <p>On all agent nodes:</p> <pre><code>systemctl stop k3s-agent.service\n</code></pre> <p>On server nodes add the following parameters in /etc/rancher/k3s/config.yaml:</p> <pre><code>nano /etc/rancher/k3s/config.yaml\nflannel-backend: none\ndisable-kube-proxy: true\n</code></pre> <p>On all k3s servers / agents:</p> <pre><code>ip link delete flannel.1\nip link delete flannel-wg\n</code></pre> <p>On all server nodes:</p> <pre><code>systemctl start k3s.service\n</code></pre> <p>On all k3s agents:</p> <pre><code>systemctl start k3s-agent.service\n</code></pre> <p>On localhost or wherever cilium cli is installed:</p>"},{"location":"operations/cilium-migration/#installing-cilium","title":"Installing cilium","text":"<p>Delete the following deployments in this order:</p> <ul> <li>kubevip-cloud-controller</li> </ul> <pre><code># The CIDR List must match the cidr range from the previous cni!\n# &lt;deployment&gt;\nhelm repo add cilium https://helm.cilium.io/\nhelm install cilium cilium/cilium --version 1.17.6 \\\n   --namespace kube-system \\\n   -f apps/argo-cd-apps-helm/values.cilium.yaml\ncilium status --wait\ncilium connectivity test\n# Cleanup cilium connectivity test\nkubectl delete ns cilium-test-1\n# Restart unmanaged pods\nkubectl get pods --all-namespaces -o custom-columns=NAMESPACE:.metadata.namespace,NAME:.metadata.name,HOSTNETWORK:.spec.hostNetwork --no-headers=true | grep '&lt;none&gt;' | awk '{print \"-n \"$1\" \"$2}' | xargs -L 1 -r kubectl delete pod\n</code></pre> <p>Reboot all nodes to re-create all host firewall rules and restart pods!</p> <p>Don't do this:</p> <pre><code>kubectl delete pods --all --all-namespaces\n</code></pre>"},{"location":"operations/longhorn-maintenance/","title":"Longhorn Maintenance Window","text":"<p>This guide describes a repeatable procedure to stop workloads in a controlled way for Longhorn upgrades and then restore normal operations.</p>"},{"location":"operations/longhorn-maintenance/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>kubectl</code> with access to the cluster</li> <li><code>yq</code> to read the maintenance plan</li> <li>Up-to-date checkout of this repository (<code>git pull</code>) so that <code>scripts/scale-maintenance-plan.yaml</code> and <code>scripts/scale-maintenance.sh</code> are available</li> </ul> <p>Note: The affected Argo CD applications ignore replica drift thanks to the <code>spec.ignoreDifferences</code> entries. During the maintenance window they will report as \"Degraded\" but will not attempt to roll replicas back automatically.</p>"},{"location":"operations/longhorn-maintenance/#procedure","title":"Procedure","text":"<ol> <li> <p>Check status <pre><code>scripts/scale-maintenance.sh status\n</code></pre>    Shows the currently configured replicas and any stored original value for each workload.</p> </li> <li> <p>Stop workloads <pre><code>scripts/scale-maintenance.sh down\n</code></pre>    - Stores the current replica count as the annotation <code>maintenance.k3s-git-ops/original-replicas</code>.    - Scales the Deployments, StatefulSets, and operator-managed CRs (Prometheus / Alertmanager) listed in <code>scripts/scale-maintenance-plan.yaml</code> down to <code>0</code>.    - The command is idempotent and can be dry-run with <code>--dry-run</code>.</p> </li> <li> <p>Perform the Longhorn upgrade    - Upgrade Longhorn (UI or Argo CD).    - Afterwards verify that the storage cluster is healthy.</p> </li> <li> <p>Restore workloads <pre><code>scripts/scale-maintenance.sh up\n</code></pre>    - Reads the stored values and restores the replicas.    - Removes the maintenance annotation afterwards.</p> </li> <li> <p>Post checks <pre><code>scripts/scale-maintenance.sh status\n</code></pre>    - Verifies that no maintenance annotations remain (empty <code>STORED</code> column).    - Trigger a \"Refresh\" in Argo CD for affected apps to update their health status.</p> </li> </ol>"},{"location":"operations/longhorn-maintenance/#adjustments","title":"Adjustments","text":"<ul> <li>Add additional workloads to <code>scripts/scale-maintenance-plan.yaml</code> (<code>id</code>, <code>namespace</code>, <code>kind</code>, <code>name</code>).</li> <li>Alternate plans (for other clusters, for example) can be provided via <code>PLAN_FILE=/path/to/plan.yaml</code> or the <code>--plan-file</code> flag.</li> <li>Use <code>--dry-run</code> for a rehearsal without actually touching replicas; the commands will be logged only.</li> </ul>"},{"location":"operations/longhorn-maintenance/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Annotation missing on restore: Confirm that the scale-down completed successfully; otherwise set the desired replica count manually (for Prometheus / Alertmanager CRs: <code>kubectl patch ... --type merge -p '{\"spec\":{\"replicas\":&lt;n&gt;}}'</code>) and re-run the script.</li> <li>Resource not found: Check namespace and name in the plan. Helm releases with prefixes must match exactly.</li> <li>Argo CD immediately restores replicas: Confirm that the application includes the <code>ignoreDifferences</code> entry and that it has been refreshed after the maintenance window.</li> </ul>"}]}